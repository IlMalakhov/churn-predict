import streamlit as st

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import holoviews as hv
import geoviews as gv
import datashader as ds
from datashader.utils import lnglat_to_meters as webm
from holoviews.operation.datashader import datashade, dynspread, rasterize
from holoviews.streams import RangeXY
from colorcet import rainbow, fire
from bokeh.models import WMTSTileSource
from streamlit_bokeh import streamlit_bokeh

import joblib
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, recall_score, precision_score, f1_score, accuracy_score, roc_curve
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb
from scipy import stats


st.set_page_config(
    page_title="Customer Churn Analytics Dashboard",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
@st.cache_data
def load_and_preprocess_data():
    customers = pd.read_csv('customers.csv')
    orders = pd.read_csv('orders.csv')
    order_payments = pd.read_csv('order_payments.csv')
    order_reviews = pd.read_csv('order_reviews.csv')
    products = pd.read_csv('products.csv')
    product_category = pd.read_csv('product_category_name_translation.csv')
    sellers = pd.read_csv('sellers.csv')
    order_items = pd.read_csv('orders_items.csv')

    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
    for df in [order_payments, order_reviews, product_category, sellers, order_items]:
        if 'Unnamed: 0' in df.columns:
            df.drop('Unnamed: 0', axis=1, inplace=True)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç –≤ orders
    date_cols = [
        'order_purchase_timestamp', 
        'order_approved_at',
        'order_delivered_carrier_date', 
        'order_delivered_customer_date',
        'order_estimated_delivery_date'
    ]

    for col in date_cols:
        orders[col] = pd.to_datetime(orders[col], format='%Y-%m-%d %H:%M:%S', errors='coerce')
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∏ –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
        if orders[col].isnull().any():
            st.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞—Ç—ã –≤ –∫–æ–ª–æ–Ω–∫–µ {col}")

    # –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ order_reviews.csv
    order_reviews = pd.read_csv('order_reviews.csv')
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –æ—Ç–∑—ã–≤–æ–≤
    order_reviews['review_creation_date'] = pd.to_datetime(
        order_reviews['review_creation_date'], 
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')

    order_reviews['review_answer_timestamp'] = pd.to_datetime(
        order_reviews['review_answer_timestamp'],
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')
    
    # –û–±–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏ (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –∏ –¥—É–±–ª–∏—Ä—É—é—â–∞—è)
    order_items['shipping_limit_date'] = pd.to_datetime(
        order_items['shipping_limit_date'],
        format='%Y-%m-%d %H:%M:%S', 
        errors='coerce')

    order_items['shipping_limit_date.1'] = pd.to_datetime(
        order_items['shipping_limit_date.1'],
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ orders –∏ customers
    orders = pd.merge(
        orders, 
        customers[['customer_id', 'customer_unique_id', 'customer_city', 'customer_state']], 
        on='customer_id', 
        how='inner'
    )
    orders.drop('customer_id', axis=1, inplace=True)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∑–∞–∫–∞–∑–æ–≤ –∫–ª–∏–µ–Ω—Ç–∞
    orders['quantity'] = orders.groupby('customer_unique_id')['order_id'].transform('nunique')

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Ç–∑—ã–≤–∞–º–∏
    merge1 = pd.merge(
        orders,
        order_reviews[['order_id', 'review_creation_date', 'review_score']],
        on='order_id',
        how='left'
    )
    
    # –£–¥–∞–ª–µ–Ω–∏–µ NA –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
    merge1.dropna(subset=['review_creation_date', 'review_score'], inplace=True)
    merge1['review_creation_date'] = pd.to_datetime(merge1['review_creation_date'])

    # –†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç—Ç–æ–∫–∞ (–≤–∞—à–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)
    analysis_date = merge1['order_purchase_timestamp'].max() + timedelta(days=1)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–ª–∏–µ–Ω—Ç—É –∏ –¥–∞—Ç–µ –∑–∞–∫–∞–∑–∞
    df_sorted = merge1.sort_values(['customer_unique_id', 'order_purchase_timestamp'])
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –∑–∞–∫–∞–∑–∞–º–∏
    df_sorted['time_diff_days'] = df_sorted.groupby('customer_unique_id')['order_purchase_timestamp'].diff().dt.days
    
    # –ú–µ–¥–∏–∞–Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –∑–∞–∫–∞–∑–∞–º–∏
    customer_intervals = df_sorted.groupby('customer_unique_id')['time_diff_days'].median().reset_index()
    customer_intervals.rename(columns={'time_diff_days': 'median_interval_days'}, inplace=True)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
    customer_stats = merge1.groupby('customer_unique_id').agg(
        last_order_date=('order_purchase_timestamp', 'max'),
        order_count=('order_id', 'nunique')
    ).reset_index()
    
    # –î–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–∫–∞–∑–∞
    customer_stats['days_since_last_order'] = (analysis_date - customer_stats['last_order_date']).dt.days
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
    customer_features = pd.merge(customer_stats, customer_intervals, on='customer_unique_id', how='left')
    
    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥ –æ—Ç—Ç–æ–∫–∞ (–≤–∞—à–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞)
    churn_threshold_multiplier = 2.0
    default_churn_days = customer_features['days_since_last_order'].mean()  # ~77 –¥–Ω–µ–π
    
    def calculate_dynamic_churn(row):
        if row['order_count'] == 1:
            return 1 if row['days_since_last_order'] > default_churn_days else 0
        elif pd.isna(row['median_interval_days']) or row['median_interval_days'] <= 0:
            return 1 if row['days_since_last_order'] > default_churn_days else 0
        else:
            personalized_threshold = row['median_interval_days'] * churn_threshold_multiplier
            return 1 if row['days_since_last_order'] > personalized_threshold else 0
    
    customer_features['is_churned'] = customer_features.apply(calculate_dynamic_churn, axis=1)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    merge1 = pd.merge(
        merge1,
        customer_features[['customer_unique_id', 'is_churned', 'days_since_last_order', 'median_interval_days']],
        on='customer_unique_id',
        how='left'
    )

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
    products = pd.merge(
        products, 
        product_category, 
        on='product_category_name', 
        how='left'
    )
    products.drop('product_category_name', axis=1, inplace=True)
    products = products.rename(columns={'product_category_name_english': 'product_category_name'})
    products['product_category_name'] = products['product_category_name'].str.strip().str.lower()
    products.dropna(subset=['product_category_name'], inplace=True)
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    category_corrections = {
        'home_confort': 'home_comfort',
        'home_appliances_2': 'home_appliances',
    }
    products['product_category_name'] = products['product_category_name'].replace(category_corrections)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –∏ –ø—Ä–æ–¥–∞–≤—Ü–∞–º–∏
    merge2 = pd.merge(
        products,
        order_items[['order_id', 'product_id', 'seller_id', 'price']],
        on='product_id',
        how='left'
    ).drop_duplicates()

    merge3 = pd.merge(
        merge2,
        sellers[['seller_id', 'seller_city', 'seller_state']],
        on='seller_id',
        how='left'
    )

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    merge4 = pd.merge(
        merge1,
        merge3,
        on='order_id',
        how='left'
    )

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–µ–π
    merge5 = pd.merge(
        merge4,
        order_payments[['order_id', 'payment_type', 'payment_installments', 'payment_value']],
        on='order_id',
        how='left'
    ).drop_duplicates()

    # –£–¥–∞–ª–µ–Ω–∏–µ NA –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
    merge5 = merge5.dropna(subset=['product_id', 'payment_type'])

    # –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    merge5['approval_delay'] = (merge5['order_approved_at'] - merge5['order_purchase_timestamp']).dt.total_seconds()
    merge5['delivery_time_to_customer_days'] = (merge5['order_delivered_customer_date'] - merge5['order_purchase_timestamp']).dt.days
    merge5['delivery_delay'] = (merge5['order_delivered_customer_date'] - merge5['order_estimated_delivery_date']).dt.days
    merge5['estimated_delivery_time'] = (merge5['order_estimated_delivery_date'] - merge5['order_purchase_timestamp']).dt.days

    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ª–æ–≥–∏—á–Ω—ã—Ö —Å—Ç—Ä–æ–∫
    merge5 = merge5[
        (merge5['order_approved_at'] < merge5['order_delivered_customer_date']) &
        (merge5['order_approved_at'] < merge5['order_delivered_carrier_date']) &
        (merge5['order_delivered_carrier_date'] < merge5['order_delivered_customer_date'])
    ]

    # RFM –∞–Ω–∞–ª–∏–∑
    rfm = merge5.groupby('customer_unique_id').agg({
        'order_purchase_timestamp': 'max',
        'order_id': 'nunique',
        'payment_value': 'sum'
    }).reset_index()
    
    rfm.columns = ['customer_unique_id', 'last_order', 'Frequency', 'MonetaryValue']
    rfm['Recency'] = (analysis_date - rfm['last_order']).dt.days
    rfm.drop('last_order', axis=1, inplace=True)
    
    # RFM Scores
    quantiles = rfm[['Recency', 'Frequency', 'MonetaryValue']].quantile(q=[0.2, 0.4, 0.6, 0.8]).to_dict()
    
    def RScore(x, p, d):
        if x <= d[p][0.2]: return 5
        elif x <= d[p][0.4]: return 4
        elif x <= d[p][0.6]: return 3
        elif x <= d[p][0.8]: return 2
        else: return 1
    
    def FMScore(x, p, d):
        if x <= d[p][0.2]: return 1
        elif x <= d[p][0.4]: return 2
        elif x <= d[p][0.6]: return 3
        elif x <= d[p][0.8]: return 4
        else: return 5
    
    rfm['R'] = rfm['Recency'].apply(RScore, args=('Recency', quantiles))
    rfm['F'] = rfm['Frequency'].apply(FMScore, args=('Frequency', quantiles))
    rfm['M'] = rfm['MonetaryValue'].apply(FMScore, args=('MonetaryValue', quantiles))
    rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ RFM —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    merge6 = pd.merge(
        merge5,
        rfm[['customer_unique_id', 'Frequency', 'MonetaryValue', 'Recency', 'R', 'F', 'M', 'RFM_Score']],
        on='customer_unique_id',
        how='left'
    )

    
    # –í—ã–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = ['customer_unique_id',        'customer_city', 'customer_state', 'order_status', 'payment_type',
        'payment_installments', 'payment_value', 'review_score', 'price',
        'product_weight_g', 'seller_city', 'seller_state', 'product_category_name',
        'approval_delay', 'delivery_time_to_customer_days', 'estimated_delivery_time',
        'delivery_delay', 'RFM_Score', 'is_churned'
    ]
    merge6.dropna(subset=features, inplace=True)
    
    return merge6[features]

@st.cache_data(show_spinner=True)
def load_and_prepare_geo():
    geo = pd.read_csv("geolocation.csv", dtype={'geolocation_zip_code_prefix': str})

    for n in range(1, 5):
        geo[f'geolocation_zip_code_prefix_{n}_digits'] = geo['geolocation_zip_code_prefix'].str[:n]

    geo = geo.query(
        "geolocation_lat <= 5.27438888 and geolocation_lng >= -73.98283055 "
        "and geolocation_lat >= -33.75116944 and geolocation_lng <= -34.79314722"
    )

    geo['x'], geo['y'] = webm(geo.geolocation_lng, geo.geolocation_lat)

    int_cols = [c for c in geo.columns if 'prefix' in c]
    geo[int_cols] = geo[int_cols].astype(int)

    return geo

@st.cache_data(show_spinner=True)
def load_orders():
    orders_df      = pd.read_csv("orders.csv")
    order_items    = pd.read_csv("orders_items.csv")
    order_reviews  = pd.read_csv("order_reviews.csv")
    customers      = pd.read_csv("customers.csv",
                                 dtype={'customer_zip_code_prefix': str})

    # customer zip prefix ‚Äì 3 digits
    customers['customer_zip_code_prefix_3_digits'] = (
        customers['customer_zip_code_prefix'].str[:3].astype(int)
    )

    # merge the four tables into one big fat orders frame
    orders = (
        orders_df
        .merge(order_items, on='order_id')
        .merge(customers,  on='customer_id')
        .merge(order_reviews, on='order_id')
    )

    return orders



# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏—á–∞–º–∏
# –î–æ–±–∞–≤–∏–º —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)
numerical_features = ['payment_value','review_score','price','product_weight_g',
                  'approval_delay', 'RFM_Score']
categorical_features = ['customer_city','customer_state','order_status','payment_type',
                      'payment_installments','seller_city','seller_state','product_category_name']

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏
def calculate_specificity(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
def extended_classification_report(y_true, y_pred, y_proba, model_name):
    metrics = {
        'specificity': calculate_specificity(y_true, y_pred),
        'sensitivity': recall_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'roc_auc': roc_auc_score(y_true, y_proba),
        'accuracy': accuracy_score(y_true, y_pred)
    }
    return metrics

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
def train_model(model, X_train, y_train):
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', model)
    ])
    pipeline.fit(X_train, y_train)
    return pipeline

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
def predict_churn(model, X):
    y_pred = model.predict(X)
    y_proba = model.predict_proba(X)[:, 1]
    return y_pred, y_proba

@st.cache_data
def load_data():
    return load_and_preprocess_data()
# –î–æ–±–∞–≤–ª—è–µ–º –≤–∫–ª–∞–¥–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π –≤ main()

def main():
    st.title("üìä –ê–Ω–∞–ª–∏–∑ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        df = load_data()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ['customer_state', 'payment_value', 'review_score', 
                          'RFM_Score', 'is_churned']
        missing_cols = [col for col in required_columns if col not in df.columns]
        
        if missing_cols:
            st.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
            st.stop()
        
        # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
        st.sidebar.header("üîç –§–∏–ª—å—Ç—Ä—ã")
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–≥–∏–æ–Ω—É
        selected_states = st.sidebar.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–≥–∏–æ–Ω—ã:",
            options=df['customer_state'].unique(),
            default=df['customer_state'].unique()
        )
        
        # –§–∏–ª—å—Ç—Ä –ø–æ RFM Score
        rfm_range = st.sidebar.slider(
            "–î–∏–∞–ø–∞–∑–æ–Ω RFM Score:",
            min_value=int(df['RFM_Score'].min()),
            max_value=int(df['RFM_Score'].max()),
            value=(int(df['RFM_Score'].min()), int(df['RFM_Score'].max()))
        )
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        filtered_df = df[
            (df['customer_state'].isin(selected_states)) &
            (df['RFM_Score'].between(rfm_range[0], rfm_range[1]))
        ]
        
        # –û—Å–Ω–æ–≤–Ω–∞—è –ø–∞–Ω–µ–ª—å - –¥–æ–±–∞–≤–ª—è–µ–º –≤–∫–ª–∞–¥–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["üìà –û–±–∑–æ—Ä", "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "üîç –î–µ—Ç–∞–ª–∏", "ü§ñ –ú–æ–¥–µ–ª–∏", "üó∫Ô∏è –ö–∞—Ä—Ç–∞"])
        
        # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏ ...
        with tab1:
            # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            col1, col2, col3 = st.columns(3)
            col1.metric("–í—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤", filtered_df['customer_unique_id'].nunique())
            col2.metric("–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞", 
                       f"{filtered_df['is_churned'].mean():.1%}",
                       help="–î–æ–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å is_churned=1")
            col3.metric("–°—Ä–µ–¥–Ω–∏–π RFM Score", 
                       f"{filtered_df['RFM_Score'].mean():.1f}")
            
            # –¢–æ–ø —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –æ—Ç—Ç–æ–∫—É
            st.subheader("–¢–æ–ø —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –æ—Ç—Ç–æ–∫—É")
            churn_by_state = filtered_df.groupby('customer_state')['is_churned'].mean().sort_values(ascending=False)
            st.bar_chart(churn_by_state)
        
        with tab2:
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RFM Score
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RFM Score")
            fig1 = px.histogram(filtered_df, x='RFM_Score', color='is_churned',
                              nbins=20, barmode='overlay')
            st.plotly_chart(fig1, use_container_width=True)
            
            # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            st.subheader("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            num_cols = ['payment_value', 'review_score', 'RFM_Score', 'is_churned']
            corr_matrix = filtered_df[num_cols].corr()
            fig2 = px.imshow(corr_matrix, text_auto=True)
            st.plotly_chart(fig2, use_container_width=True)
        
        with tab3:
            # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
            st.subheader("–î–µ—Ç–∞–ª–∏ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º")
            
            # –í—ã–±–æ—Ä –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            selected_customer = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∏–µ–Ω—Ç–∞:",
                options=filtered_df['customer_unique_id'].unique()
            )
            
            # –î–∞–Ω–Ω—ã–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
            customer_data = filtered_df[filtered_df['customer_unique_id'] == selected_customer].iloc[0]
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            col1, col2 = st.columns(2)
            with col1:
                st.metric("–†–µ–≥–∏–æ–Ω", customer_data['customer_state'])
                st.metric("RFM Score", customer_data['RFM_Score'])
                st.metric("–°—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫", f"{customer_data['payment_value']:.2f}")
            
            with col2:
                st.metric("–û—Ü–µ–Ω–∫–∞ –æ—Ç–∑—ã–≤–∞", customer_data['review_score'])
                is_churned = customer_data['is_churned'] == 1
                churn_label = "–î–∞" if is_churned else "–ù–µ—Ç"
                delta_text = "–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫" if is_churned else "–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫"
                st.metric("–°—Ç–∞—Ç—É—Å –æ—Ç—Ç–æ–∫–∞", 
                         churn_label,
                         delta=delta_text,
                         delta_color="inverse" if is_churned else "normal"
                         )
    
  
        with tab4:
            st.header("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            X = filtered_df[numerical_features + categorical_features]
            y = filtered_df['is_churned']
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y)
            
            # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            model_option = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
                ("Logistic Regression", "Decision Tree", "XGBoost")
            )
            
            if st.button("–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å"):
                with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                    if model_option == "Logistic Regression":
                        model = LogisticRegression(
                            class_weight='balanced',
                            max_iter=1000,
                            random_state=42
                        )
                    elif model_option == "Decision Tree":
                        model = DecisionTreeClassifier(
                            max_depth=5,
                            min_samples_leaf=50,
                            class_weight='balanced',
                            random_state=42
                        )
                    elif model_option == "XGBoost":
                        scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)
                        model = xgb.XGBClassifier(
                            objective='binary:logistic',
                            n_estimators=150,
                            max_depth=5,
                            learning_rate=0.1,
                            subsample=0.8,
                            colsample_bytree=0.8,
                            scale_pos_weight=scale_pos_weight,
                            random_state=42,
                            n_jobs=-1
                        )
                    
                    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    pipeline = train_model(model, X_train, y_train)
                    st.session_state.trained_pipeline = pipeline  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–µ—Å—Å–∏—é
                    st.session_state.model_option = model_option # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º—è –º–æ–¥–µ–ª–∏

                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    y_pred, y_proba = predict_churn(pipeline, X_test)
                    
                    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
                    metrics = extended_classification_report(y_test, y_pred, y_proba, model_option)
                    
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                    st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏")
                    col1, col2, col3 = st.columns(3)
                    col1.metric("ROC-AUC", f"{metrics['roc_auc']:.4f}")
                    col2.metric("F1 Score", f"{metrics['f1']:.4f}")
                    col3.metric("Accuracy", f"{metrics['accuracy']:.4f}")
                    
                    col4, col5, col6 = st.columns(3)
                    col4.metric("Sensitivity", f"{metrics['sensitivity']:.4f}")
                    col5.metric("Specificity", f"{metrics['specificity']:.4f}")
                    col6.metric("Precision", f"{metrics['precision']:.4f}")
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ROC-–∫—Ä–∏–≤–æ–π
                    st.subheader("ROC-–∫—Ä–∏–≤–∞—è")
                    fpr, tpr, _ = roc_curve(y_test, y_proba)
                    fig = px.line(x=fpr, y=tpr, 
                                 labels={'x': 'False Positive Rate', 'y': 'True Positive Rate'},
                                 title=f'ROC Curve (AUC = {metrics["roc_auc"]:.2f})')
                    fig.add_shape(type='line', line=dict(dash='dash'),
                                x0=0, x1=1, y0=0, y1=1)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
                    st.subheader("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
                    cm = confusion_matrix(y_test, y_pred)
                    fig_cm = px.imshow(cm, 
                                       labels=dict(x="Predicted", y="Actual"),
                                       x=['Not Churned', 'Churned'],
                                       y=['Not Churned', 'Churned'],
                                       text_auto=True)
                    st.plotly_chart(fig_cm, use_container_width=True)
                    
                    st.success(f"–ú–æ–¥–µ–ª—å {model_option} —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")

            if 'trained_pipeline' in st.session_state:
                st.subheader("–°–∫–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
                pipeline_to_save = st.session_state.trained_pipeline
                model_option_to_save = st.session_state.model_option
                model_filename = f'{model_option_to_save.replace(" ", "_").lower()}_model.pkl'
                
                # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                from io import BytesIO
                model_bytes = BytesIO()
                joblib.dump(pipeline_to_save, model_bytes)
                model_bytes.seek(0)

                st.download_button(
                    label=f"–°–∫–∞—á–∞—Ç—å {model_option_to_save.replace(" ", "_")}.pkl",
                    data=model_bytes,
                    file_name=model_filename,
                    mime="application/octet-stream"
                )

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
            st.subheader("–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞")
            customer_id_predict = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞:",
                options=filtered_df['customer_unique_id'].unique(),
                key="customer_predict_select" # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            )
            
            if st.button("–°–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑"):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤ —Å–µ—Å—Å–∏–∏
                if 'trained_pipeline' not in st.session_state:
                    st.error("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–æ–π –≤–∫–ª–∞–¥–∫–µ.")
                else:
                    try:
                        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ —Å–µ—Å—Å–∏–∏
                        pipeline = st.session_state.trained_pipeline
                        current_model_option = st.session_state.model_option
                        
                        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–∞
                        customer_data = filtered_df[filtered_df['customer_unique_id'] == customer_id_predict].iloc[0:1]
                        X_customer = customer_data[numerical_features + categorical_features]
                        
                        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                        pred, proba = predict_churn(pipeline, X_customer)
                        
                        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞")
                        col1, col2 = st.columns(2)
                        col1.metric("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞", f"{proba[0]:.2%}")

                        is_high_risk = pred[0] == 1
                        prognosis_label = "–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞" if is_high_risk else "–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞"
                        delta_display_text = "‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ" if is_high_risk else "‚úÖ –ù–æ—Ä–º–∞"

                        col2.metric("–ü—Ä–æ–≥–Ω–æ–∑",
                                   prognosis_label,
                                   delta=delta_display_text,
                                   # Set color explicitly: red for high risk, green for low risk
                                   delta_color="inverse" if is_high_risk else "normal"
                                   )

                        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π)
                        if current_model_option in ["Decision Tree", "XGBoost"]:
                            st.subheader("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                            try:
                                # –ü–æ–ª—É—á–∞–µ–º –æ–±—É—á–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞
                                classifier = pipeline.named_steps['classifier']
                                
                                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
                                preprocessor_step = pipeline.named_steps['preprocessor']
                                
                                # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ OneHotEncoding
                                ohe_feature_names = preprocessor_step.transformers_[1][1]\
                                    .get_feature_names_out(categorical_features)
                                
                                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞
                                feature_names = numerical_features + list(ohe_feature_names)
                                
                                # –ü–æ–ª—É—á–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                                importances = classifier.feature_importances_
                                
                                # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                                importance_df = pd.DataFrame({
                                    'feature': feature_names,
                                    'importance': importances
                                }).sort_values(by='importance', ascending=False)
                                
                                # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–æ–ø-N –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                                top_n = 15
                                fig_importance = px.bar(
                                    importance_df.head(top_n),
                                    x='importance',
                                    y='feature',
                                    orientation='h',
                                    title=f'–¢–æ–ø-{top_n} –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {current_model_option}'
                                )
                                st.plotly_chart(fig_importance, use_container_width=True)
                                
                            except AttributeError:
                                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {current_model_option}.")
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")

                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏: {str(e)}")

            with tab5:
                st.subheader("–î–æ—Ö–æ–¥ –æ—Ç –∑–∞–∫–∞–∑–æ–≤ (—Ç—ã—Å. —Ä–µ–∞–ª–æ–≤)")

                hv.extension('bokeh', logo=False)

                overlay_opts = dict(width=800, height=600, toolbar='above', xaxis=None, yaxis=None)
                quad_opts = dict(tools=['hover'], colorbar=True, alpha=0, hover_alpha=0.2)
                gv.opts.defaults(
                    gv.opts.Overlay(**overlay_opts),
                    gv.opts.QuadMesh(**quad_opts)
                )

                brazil = load_and_prepare_geo()

                # column the aggregation will use
                agg_name = 'geolocation_zip_code_prefix'

                # ---------------------------------------------------------------------------------
                # 2. holoviews + datashader plot builder
                # ---------------------------------------------------------------------------------
                T  = 0.05      # dynspread threshold
                PX = 1         # dynspread px growth

                def plotted_zipcodes(df, agg_name=agg_name, cmap=rainbow):
                    # background tiles (dark grey canvas)
                    url = (
                        "https://server.arcgisonline.com/ArcGIS/rest/services/"
                        "Canvas/World_Dark_Gray_Base/MapServer/tile/{Z}/{Y}/{X}.png"
                    )
                    geomap  = gv.WMTS(WMTSTileSource(url=url))

                    # points in projected coords
                    points  = hv.Points(gv.Dataset(df, kdims=['x', 'y'], vdims=[agg_name]))

                    # datashader aggregation
                    agg     = datashade(points,
                                        element_type=gv.Image,
                                        aggregator=ds.min(agg_name),   # same as the legacy call
                                        cmap=cmap)

                    # tweak visibility
                    zipcodes = dynspread(agg, threshold=T, max_px=PX)

                    # interactive hover layer (the small one keeps perf ok)
                    hover = hv.util.Dynamic(
                        rasterize(points,
                                aggregator=ds.min(agg_name),
                                width=50,
                                height=25,
                                streams=[RangeXY]
                        ),
                        operation=hv.QuadMesh
                    ).opts(cmap=cmap)

                    return geomap * zipcodes * hover

                with st.spinner('–ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Ä—Ç—É...'):

                    bokeh_fig = hv.render(plotted_zipcodes(brazil), backend='bokeh')

                # streamlit_bokeh(bokeh_fig, use_container_width=True)

                def filter_data(df, level, name):

                    df = df[df[level] == name]
                    df = df[(df.x <= df.x.quantile(0.999)) & (df.x >= df.x.quantile(0.001))]
                    df = df[(df.y <= df.y.quantile(0.999)) & (df.y >= df.y.quantile(0.001))]
                    return df
                

                def build_revenue_df(geo, orders):

                    geo3 = geo.set_index('geolocation_zip_code_prefix_3_digits').copy()


                    gp = orders.groupby('customer_zip_code_prefix_3_digits')['price'].sum()

                    revenue = geo3.join(gp)
                    revenue['revenue'] = revenue['price'].fillna(0) / 1_000

                    return revenue
                
                def map_plot(df, agg_name='revenue', cmap=fire):
 
                    T, PX = 0.05, 1
                    url = (
                        "https://server.arcgisonline.com/ArcGIS/rest/services/"
                        "Canvas/World_Dark_Gray_Base/MapServer/tile/{Z}/{Y}/{X}.png"
                    )
                    geomap = gv.WMTS(WMTSTileSource(url=url))

                    points = hv.Points(gv.Dataset(df, kdims=['x', 'y'], vdims=[agg_name]))

                    agg = datashade(points,
                                    element_type=gv.Image,
                                    aggregator=ds.mean(agg_name),
                                    cmap=cmap)
                    img = dynspread(agg, threshold=T, max_px=PX)

                    hover = hv.util.Dynamic(
                        rasterize(points,
                                aggregator=ds.mean(agg_name),
                                width=50, height=25,
                                streams=[RangeXY]),
                        operation=hv.QuadMesh
                    ).opts(cmap=cmap)

                    return geomap * img * hover

                geo = load_and_prepare_geo()
                orders = load_orders()

                st.sidebar.header("üìç –ö–∞—Ä—Ç–∞")
                states = ['<all brazil>'] + sorted(geo['geolocation_state'].unique())
                state_choice = st.sidebar.selectbox("state (uf)", states, index=0)

                cities_in_state = sorted(geo.query("geolocation_state == @state_choice")['geolocation_city'].str.lower().unique())

                sub_geo = filter_data(geo, 'geolocation_state', state_choice)

                if state_choice == '<all brazil>':
                    sub_geo    = geo.copy()                     
                    sub_orders = orders.copy()                  
                else:
                    sub_geo    = filter_data(geo, 'geolocation_state', state_choice)
                    sub_orders = orders[orders['customer_state'] == state_choice]

                    cities_in_state = sorted(
                        sub_geo['geolocation_city'].str.lower().unique()
                    )
                    city_choice = st.sidebar.selectbox(
                        "city (optional)", ['<all>'] + cities_in_state
                    )

                    if city_choice != '<all>':
                        sub_geo = filter_data(sub_geo, 'geolocation_city', city_choice)
                        sub_orders = sub_orders[
                            sub_orders['customer_city'].str.lower() == city_choice
                        ]

                revenue_df = build_revenue_df(sub_geo, sub_orders)

                st.sidebar.markdown("### –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
                st.sidebar.metric("n¬∫ zip prefixes", len(revenue_df))
                st.sidebar.metric("total revenue (k R$)", int(revenue_df['revenue'].sum()))

                if st.sidebar.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å `describe()`", value=False):
                    st.sidebar.dataframe(revenue_df['revenue'].describe().to_frame(), use_container_width=True)

                with st.spinner("–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ datashader tiles"):
                    fig = hv.render(map_plot(revenue_df), backend='bokeh')
                streamlit_bokeh(fig, use_container_width=True)



    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        st.stop()

if __name__ == "__main__":
    main()