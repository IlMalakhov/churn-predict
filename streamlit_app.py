# ==============================================
# –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
# ==============================================

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px


import holoviews as hv
import geoviews as gv
import datashader as ds
from datashader.utils import lnglat_to_meters as webm
from holoviews.operation.datashader import datashade, dynspread, rasterize
from holoviews.streams import RangeXY
from colorcet import rainbow, fire, bgy, kr
from bokeh.models import WMTSTileSource
from streamlit_bokeh import streamlit_bokeh

import joblib
from datetime import datetime, timedelta
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    classification_report, roc_auc_score, confusion_matrix, 
    recall_score, precision_score, f1_score, accuracy_score, roc_curve
)
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
import xgboost as xgb
from scipy import stats
from scipy.special import boxcox1p

# ==============================================
# –ù–ê–°–¢–†–û–ô–ö–ê –°–¢–†–ê–ù–ò–¶–´ STREAMLIT
# ==============================================
st.set_page_config(
    page_title="Customer Churn Analytics Dashboard",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==============================================
# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•
# ==============================================

@st.cache_data
def load_and_preprocess_data() -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ CSV-—Ñ–∞–π–ª—ã, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –±–∞–∑–æ–≤—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É,
    –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏, –≤–∫–ª—é—á–∞—è RFM –∏ —Å—Ç–∞—Ç—É—Å –æ—Ç—Ç–æ–∫–∞.

    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: –ö–æ—Ä—Ç–µ–∂ –∏–∑ –¥–≤—É—Ö DataFrame:
            - –û—Å–Ω–æ–≤–Ω–æ–π DataFrame —Å–æ –≤—Å–µ–º–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
            - DataFrame —Å RFM-–º–µ—Ç—Ä–∏–∫–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞.
    """
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–æ–≤
    customers = pd.read_csv('data/customers.csv')
    orders = pd.read_csv('data/orders.csv')
    order_payments = pd.read_csv('data/order_payments.csv')
    order_reviews = pd.read_csv('data/order_reviews.csv')
    products = pd.read_csv('data/products.csv')
    product_category = pd.read_csv('data/product_category_name_translation.csv')
    sellers = pd.read_csv('data/sellers.csv')
    order_items = pd.read_csv('data/orders_items.csv')

    for df in [order_payments, order_reviews, product_category, sellers, order_items]:
        if 'Unnamed: 0' in df.columns:
            df.drop('Unnamed: 0', axis=1, inplace=True)

    date_cols = [
        'order_purchase_timestamp',
        'order_approved_at',
        'order_delivered_carrier_date',
        'order_delivered_customer_date',
        'order_estimated_delivery_date'
    ]

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤ —Å –¥–∞—Ç–∞–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç datetime
    for col in date_cols:
        orders[col] = pd.to_datetime(orders[col], format='%Y-%m-%d %H:%M:%S', errors='coerce')
        invalid_count = orders[col].isnull().sum()

    order_reviews = pd.read_csv('data/order_reviews.csv')
    order_reviews['review_creation_date'] = pd.to_datetime(
        order_reviews['review_creation_date'], 
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')

    order_reviews['review_answer_timestamp'] = pd.to_datetime(
        order_reviews['review_answer_timestamp'],
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')
    
    order_items['shipping_limit_date'] = pd.to_datetime(
        order_items['shipping_limit_date'],
        format='%Y-%m-%d %H:%M:%S', 
        errors='coerce')

    order_items['shipping_limit_date.1'] = pd.to_datetime(
        order_items['shipping_limit_date.1'],
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–∫–∞–∑–∞—Ö –∏ –∫–ª–∏–µ–Ω—Ç–∞—Ö
    orders = pd.merge(
        orders,
        customers[['customer_id', 'customer_unique_id', 'customer_city', 'customer_state']],
        on='customer_id',
        how='inner'
    )
    orders.drop('customer_id', axis=1, inplace=True)

    orders['quantity'] = orders.groupby('customer_unique_id')['order_id'].transform('nunique')

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑–æ–≤ —Å –æ—Ç–∑—ã–≤–∞–º–∏
    merge1 = pd.merge(
        orders,
        order_reviews[['order_id', 'review_creation_date', 'review_score']],
        on='order_id',
        how='left'
    )

    #merge1.dropna(subset=['review_creation_date', 'review_score'], inplace=True)
    merge1.dropna(inplace=True)
    merge1['review_creation_date'] = pd.to_datetime(merge1['review_creation_date'])

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –∑–∞–∫–∞–∑–∞ + 1 –¥–µ–Ω—å)
    analysis_date = merge1['order_purchase_timestamp'].max() + timedelta(days=1)

    ####################################################################################################################################################
    
    df_sorted = merge1.sort_values(['customer_unique_id', 'order_purchase_timestamp'])
    # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∏–µ–Ω—Ç—É –∏ –≤—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –∑–∞–∫–∞–∑–∞–º–∏
    def calculate_avg_time_between_orders(group):
        if len(group) > 1:
            time_diffs = group['order_purchase_timestamp'].diff().dt.days  # —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–Ω—è—Ö
            return time_diffs.median() #–º–µ–¥–∏–∞–Ω–∞
        else:
            return pd.NA  # –µ—Å–ª–∏ –∑–∞–∫–∞–∑ –æ–¥–∏–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º NA
    
    avg_time_between_orders = (
        df_sorted
        .groupby('customer_unique_id')
        .apply(calculate_avg_time_between_orders)
        .reset_index(name='avg_days_between_orders')
    )
    
    threshold = avg_time_between_orders.avg_days_between_orders.mean() # —Å—Ä–µ–¥–Ω–µ–µ - 77, –º–µ–¥–∏–∞–Ω–∞ - 25
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –≤—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É –¥–∞—Ç–∞–º–∏ –∑–∞–∫–∞–∑–æ–≤
    df_sorted['time_diff_days'] = df_sorted.groupby('customer_unique_id')['order_purchase_timestamp'].diff().dt.days
    
    # –°—á–∏—Ç–∞–µ–º –º–µ–¥–∏–∞–Ω—É –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –º–µ–∂–¥—É –∑–∞–∫–∞–∑–∞–º–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∏–µ–Ω—Ç—É ‚Äî –æ–Ω–∞ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç –∏—Ö —á–∞—Å—Ç–æ—Ç—É –ø–æ–∫—É–ø–æ–∫
    customer_intervals = df_sorted.groupby('customer_unique_id')['time_diff_days'].median().reset_index()
    customer_intervals.rename(columns={'time_diff_days': 'median_interval_days'}, inplace=True)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –∑–∞–∫–∞–∑–∞ (last_order_date) –∏ –æ–±—â–µ–µ —á–∏—Å–ª–æ –∑–∞–∫–∞–∑–æ–≤ (order_count) (—Å–≤–æ–¥–∫–∞ –ø–æ –∫–ª–∏–µ–Ω—Ç—É)
    customer_stats = merge1.groupby('customer_unique_id').agg(
        last_order_date=('order_purchase_timestamp', 'max'),
        order_count=('order_id', 'nunique')
    ).reset_index()
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π —Å –º–æ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–∫–∞–∑–∞
    customer_stats['days_since_last_order'] = (analysis_date - customer_stats['last_order_date']).dt.days
    
    customer_features = pd.merge(customer_stats, customer_intervals, on='customer_unique_id', how='left')
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç—Ç–æ–∫–∞
    churn_threshold_multiplier = 2.0 # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –Ω—É–∂–Ω–æ –≤—ã–∂–¥–∞—Ç—å —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª, –ø—Ä–µ–∂–¥–µ —á–µ–º –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞
    default_churn_days = threshold # –ü–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–æ–≤—ã–º –∑–∞–∫–∞–∑–æ–º
    
    def calculate_dynamic_churn(row):
        if row['order_count'] == 1:
            # –ï—Å–ª–∏ —É –∫–ª–∏–µ–Ω—Ç–∞ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∑–∞–∫–∞–∑, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π threshold –∫–∞–∫ –ø–æ—Ä–æ–≥
            return 1 if row['days_since_last_order'] > default_churn_days else 0
        elif pd.isna(row['median_interval_days']) or row['median_interval_days'] <= 0:
             # –ï—Å–ª–∏ —É –∫–ª–∏–µ–Ω—Ç–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–∫–∞–∑–æ–≤, –Ω–æ –Ω–µ–ø–æ–ª–Ω—ã–µ/–∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã ‚Äî —Ç–∞–∫–∂–µ fallback –Ω–∞ –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥.
             return 1 if row['days_since_last_order'] > default_churn_days else 0
        else:
            # –ò–Ω–∞—á–µ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥: –º–µ–¥–∏–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª * –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç, –≥–¥–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç (churn_threshold_multiplier = 2.0) –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å.
            personalized_threshold = row['median_interval_days'] * churn_threshold_multiplier
            return 1 if row['days_since_last_order'] > personalized_threshold else 0
    
    customer_features['is_churned'] = customer_features.apply(calculate_dynamic_churn, axis=1)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–º
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    #churn_data_to_merge = customer_features[['customer_unique_id', 'is_churned', 'days_since_last_order', 'median_interval_days']]
    
    merge1 = pd.merge(
        merge1,
        customer_features[['customer_unique_id', 'is_churned', 'days_since_last_order', 'median_interval_days']],
        on='customer_unique_id',
        how='left'
    )
##########################################################################################################################################################
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
    products = pd.merge(
        products, 
        product_category, 
        on='product_category_name', 
        how='left'
    )
    products.drop('product_category_name', axis=1, inplace=True)
    products = products.rename(columns={'product_category_name_english': 'product_category_name'})
    products['product_category_name'] = products['product_category_name'].str.strip().str.lower()
    products.dropna(subset=['product_category_name'], inplace=True)

    category_corrections = {
        'home_confort': 'home_comfort',
        'home_appliances_2': 'home_appliances',
    }
    products['product_category_name'] = products['product_category_name'].replace(category_corrections)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –∑–∞–∫–∞–∑–∞
    merge2 = pd.merge(
        products,
        order_items[['order_id', 'product_id', 'seller_id', 'price']],
        on='product_id',
        how='left'
    ).drop_duplicates()

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ø—Ä–æ–¥–∞–≤—Ü–∞—Ö
    merge3 = pd.merge(
        merge2,
        sellers[['seller_id', 'seller_city', 'seller_state']],
        on='seller_id',
        how='left'
    )

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º DataFrame (merge1)
    merge4 = pd.merge(
        merge1,
        merge3,
        on='order_id',
        how='left'
    )

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–± –æ–ø–ª–∞—Ç–µ
    merge5 = pd.merge(
        merge4,
        order_payments[['order_id', 'payment_type', 'payment_installments', 'payment_value']],
        on='order_id',
        how='left'
    ).drop_duplicates()

    merge5 = merge5.dropna(subset=['product_id', 'payment_type'])

    # –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–æ—Å—Ç–∞–≤–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–∫–∞–∑–∞
    merge5['approval_delay'] = (merge5['order_approved_at'] - merge5['order_purchase_timestamp']).dt.total_seconds()
    merge5['delivery_time_to_customer_days'] = (merge5['order_delivered_customer_date'] - merge5['order_purchase_timestamp']).dt.days
    merge5['delivery_delay'] = (merge5['order_delivered_customer_date'] - merge5['order_estimated_delivery_date']).dt.days
    merge5['estimated_delivery_time'] = (merge5['order_estimated_delivery_date'] - merge5['order_purchase_timestamp']).dt.days

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
    merge5 = merge5[
        (merge5['order_approved_at'] < merge5['order_delivered_customer_date']) &
        (merge5['order_approved_at'] < merge5['order_delivered_carrier_date']) &
        (merge5['order_delivered_carrier_date'] < merge5['order_delivered_customer_date'])
    ]

    # –†–∞—Å—á–µ—Ç RFM –º–µ—Ç—Ä–∏–∫ (Recency, Frequency, MonetaryValue)
    rfm = merge5.groupby('customer_unique_id').agg({
        'order_purchase_timestamp': 'max', # Recency (–¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∫—É–ø–∫–∏)
        'order_id': 'nunique',            # Frequency (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–∫–∞–∑–æ–≤)
        'payment_value': 'sum'            # MonetaryValue (—Å—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫)
    }).reset_index()

    rfm.columns = ['customer_unique_id', 'last_order', 'Frequency', 'MonetaryValue']
    rfm['Recency'] = (analysis_date - rfm['last_order']).dt.days
    rfm.drop('last_order', axis=1, inplace=True)

    quantiles = rfm[['Recency', 'Frequency', 'MonetaryValue']].quantile(q=[0.2, 0.4, 0.6, 0.8]).to_dict()

    def RScore(x, p, d) -> int:
        """–ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç R-—Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞—á–µ–Ω–∏—è Recency –∏ –∫–≤–∞–Ω—Ç–∏–ª–µ–π."""
        if x <= d[p][0.2]: return 5
        elif x <= d[p][0.4]: return 4
        elif x <= d[p][0.6]: return 3
        elif x <= d[p][0.8]: return 2
        else: return 1

    def FMScore(x, p, d) -> int:
        """–ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç F –∏ M —Å–∫–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞—á–µ–Ω–∏–π Frequency/MonetaryValue –∏ –∫–≤–∞–Ω—Ç–∏–ª–µ–π."""
        if x <= d[p][0.2]: return 1
        elif x <= d[p][0.4]: return 2
        elif x <= d[p][0.6]: return 3
        elif x <= d[p][0.8]: return 4
        else: return 5

    rfm['R'] = rfm['Recency'].apply(RScore, args=('Recency', quantiles))
    rfm['F'] = rfm['Frequency'].apply(FMScore, args=('Frequency', quantiles))
    rfm['M'] = rfm['MonetaryValue'].apply(FMScore, args=('MonetaryValue', quantiles))
    rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ RFM –º–µ—Ç—Ä–∏–∫ —Å –æ—Å–Ω–æ–≤–Ω—ã–º DataFrame
    merge6 = pd.merge(
        merge5,
        rfm[['customer_unique_id', 'Frequency', 'MonetaryValue', 'Recency', 'R', 'F', 'M', 'RFM_Score']],
        on='customer_unique_id',
        how='left'
    )

    features = ['customer_unique_id', 'customer_city', 'customer_state', 'order_status', 'payment_type',
        'payment_installments', 'payment_value', 'review_score', 'price',
        'product_weight_g', 'seller_city', 'seller_state', 'product_category_name',
        'approval_delay', 'delivery_time_to_customer_days', 'estimated_delivery_time',
        'delivery_delay', 'RFM_Score', 'is_churned'
    ]

    merge6.dropna(subset=features, inplace=True)

    return merge6[features], rfm, merge6

@st.cache_data(show_spinner="üåé –∑–∞–≥—Ä—É–∂–∞–µ–º geo ‚Ä¶")
def load_geo():
    geo = pd.read_csv("data/geolocation.csv", dtype={"geolocation_zip_code_prefix": str})
    for n in range(1, 5):
        geo[f"geolocation_zip_code_prefix_{n}_digits"] = geo["geolocation_zip_code_prefix"].str[:n]
    geo = geo.query(
        "geolocation_lat <= 5.27438888 and geolocation_lng >= -73.98283055 and "
        "geolocation_lat >= -33.75116944 and geolocation_lng <= -34.79314722"
    )
    geo["x"], geo["y"] = webm(geo.geolocation_lng, geo.geolocation_lat)
    geo[[c for c in geo.columns if "prefix" in c]] = geo.filter(like="prefix").astype(int)
    return geo

@st.cache_data(show_spinner="üì¶ –∑–∞–≥—Ä—É–∂–∞–µ–º orders ‚Ä¶")
def load_orders():
    o   = pd.read_csv("data/orders.csv")
    it  = pd.read_csv("data/orders_items.csv")
    rv  = pd.read_csv("data/order_reviews.csv")
    cu  = pd.read_csv("data/customers.csv", dtype={"customer_zip_code_prefix": str})
    cu["customer_zip_code_prefix_3_digits"] = cu["customer_zip_code_prefix"].str[:3].astype(int)
    return (
        o.merge(it, on="order_id").merge(cu, on="customer_id").merge(rv, on="order_id")
    )

# ==============================================
# –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–ï–õ–ï–ô –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
# ==============================================

numerical_features = ['payment_value','review_score','price','product_weight_g',
                  'approval_delay', 'RFM_Score']
categorical_features = ['customer_city','customer_state','order_status','payment_type',
                      'payment_installments','seller_city','seller_state','product_category_name']

def calculate_specificity(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å (True Negative Rate) –º–æ–¥–µ–ª–∏.

    Args:
        y_true (np.ndarray): –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤.
        y_pred (np.ndarray): –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤.

    Returns:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏.
    """
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

    if (tn + fp) == 0:
        return 0.0
    return tn / (tn + fp)

def extended_classification_report(y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray, model_name: str) -> dict:
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

    Args:
        y_true (np.ndarray): –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤.
        y_pred (np.ndarray): –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤.
        y_proba (np.ndarray): –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∫–ª–∞—Å—Å—É 1.
        model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏).

    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.
    """
    metrics = {
        'specificity': calculate_specificity(y_true, y_pred),
        'sensitivity': recall_score(y_true, y_pred), # Recall
        'precision': precision_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'roc_auc': roc_auc_score(y_true, y_proba),
        'accuracy': accuracy_score(y_true, y_pred)
    }
    return metrics

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features), # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # One-Hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    ])

def train_model(model, X_train: pd.DataFrame, y_train: pd.Series) -> Pipeline:
    """
    –°–æ–∑–¥–∞–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω —Å –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º –∏ –º–æ–¥–µ–ª—å—é, –æ–±—É—á–∞–µ—Ç –µ–≥–æ.

    Args:
        model: –≠–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, LogisticRegression).
        X_train (pd.DataFrame): –û–±—É—á–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
        y_train (pd.Series): –û–±—É—á–∞—é—â–∏–µ —Ü–µ–ª–µ–≤—ã–µ –º–µ—Ç–∫–∏.

    Returns:
        Pipeline: –û–±—É—á–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω Scikit-learn.
    """
    pipeline = Pipeline([
        ('preprocessor', preprocessor), # –®–∞–≥ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        ('classifier', model)          # –®–∞–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    ])
    pipeline.fit(X_train, y_train) # –û–±—É—á–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
    return pipeline

def predict_churn(model: Pipeline, X: pd.DataFrame) -> tuple[np.ndarray, np.ndarray]:
    """
    –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞.

    Args:
        model (Pipeline): –û–±—É—á–µ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω.
        X (pd.DataFrame): –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.

    Returns:
        tuple[np.ndarray, np.ndarray]: –ö–æ—Ä—Ç–µ–∂ –∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∫–ª–∞—Å—Å–∞ 1.
    """
    y_pred = model.predict(X) # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    y_proba = model.predict_proba(X)[:, 1] # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∫–ª–∞—Å—Å–∞ 1
    return y_pred, y_proba

@st.cache_data
def load_data() -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.

    Returns:
        tuple[pd.DataFrame, pd.DataFrame]: –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è load_and_preprocess_data().
    """
    return load_and_preprocess_data()

# ==============================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ó–ê–ü–£–°–ö–ê –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ==============================================
def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∑–∞–ø—É—Å–∫–∞—é—â–∞—è Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç—Ç–æ–∫–∞.
    –í–∫–ª—é—á–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –∏ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ.
    """

    from pathlib import Path
    import base64

    logo_path = Path("assets/logo_white_font.png")
    logo_base64 = base64.b64encode(logo_path.read_bytes()).decode()

    st.markdown(
    f"""
    <style>
        .logo-corner {{
            position: fixed;
            right: 1rem;
            bottom: 1rem;
            width: 100px;
            cursor: pointer;
            opacity: 0.4;
            z-index: 100;
            transition: all 0.3s ease;
            }}
        
        .logo-corner:hover {{
            transform: scale(1.1);
            opacity: 1;
        }}

        .logo-corner:active {{
            transform: scale(0.9);
            opacity: 0.65;
            }}
    </style>

    <img class="logo-corner"
        src="data:image/png;base64,{logo_base64}">
    """,
    unsafe_allow_html=True,
    )

    st.title("–ê–Ω–∞–ª–∏–∑ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")

    try:
        df = load_data()[0]
        
        required_columns = ['customer_state', 'payment_value', 'review_score', 
                          'RFM_Score', 'is_churned']
        missing_cols = [col for col in required_columns if col not in df.columns]
        
        if missing_cols:
            st.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
            st.stop()
        
        st.sidebar.header("üîç –§–∏–ª—å—Ç—Ä—ã")
        
        # –ø–æ–ª—É—á–∞–µ–º –≤—Å–µ —à—Ç–∞—Ç—ã
        all_states = sorted(df['customer_state'].unique())
        # –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º session_state –µ–¥–∏–Ω–æ–∂–¥—ã
        if 'selected_states' not in st.session_state:
            st.session_state.selected_states = all_states.copy()

        if st.sidebar.button("–í—ã–±—Ä–∞—Ç—å –≤—Å–µ", key="select_all"):
            st.session_state.selected_states = all_states.copy()
            st.rerun()

        # —Å–∞–º multiselect —Ç–µ–ø–µ—Ä—å –Ω–∏–∂–µ
        selected_states = st.sidebar.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ —à—Ç–∞—Ç—ã:",
            options=all_states,
            key="selected_states"
        )
        
        rfm_range = st.sidebar.slider(
            "–î–∏–∞–ø–∞–∑–æ–Ω RFM Score:",
            min_value=int(df['RFM_Score'].min()),
            max_value=int(df['RFM_Score'].max()),
            value=(int(df['RFM_Score'].min()), int(df['RFM_Score'].max()))
        )
        
        filtered_df = df[
            (df['customer_state'].isin(selected_states)) &
            (df['RFM_Score'].between(rfm_range[0], rfm_range[1]))
        ]

        # –≠—Ç–æ —à—Ç—É–∫–∞ –¥–µ–ª–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤–∫–ª–∞–¥–æ–∫ –±–æ–ª—å—à–µ
        st.markdown(
        """
        <style>
        /* Target the tab buttons and their inner text */
        div[data-baseweb="tab-list"] button[role="tab"] > div {
            font-size: 20px !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
        )

        st.markdown(
        """
        <style>
        /* Target the tab list container to enable flexbox and take full width */
        div[data-baseweb="tab-list"] {
            display: flex;
            width: 100%;
        }

        /* Target individual tab buttons to make them grow equally and center text */
        div[data-baseweb="tab-list"] button[role="tab"] {
            flex-grow: 1;
            justify-content: center; /* Center the text inside the tab button */
        }

        /* Target the inner div containing the text for font size */
        div[data-baseweb="tab-list"] button[role="tab"] > div {
            font-size: 20px !important; /* Keep existing font size rule */
        }
        </style>
        """,
        unsafe_allow_html=True,
        )
        
        tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs(["–û–±–∑–æ—Ä", "–î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –∫–∞—Ä—Ç–∞", "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "–î–µ—Ç–∞–ª–∏", "–ú–æ–¥–µ–ª–∏", "–ö–∞—Ä—Ç–∞", "Power BI"])
        
        # –í–∫–ª–∞–¥–∫–∞ 1: –û–±–∑–æ—Ä
        with tab1:
            st.markdown("""
            <style>
            * {
                scroll-behavior: smooth;
            }
            .toc-box {
                border: 1px solid #FFFFFF;
                border-radius: 8px;
                padding: 24px; 
                margin-top: 24px;
                margin-bottom: 36px;
                background-color: #0f1116;
            }
            .toc-box h4 {
                margin-top: 0;
                margin-bottom: 10px;
                font-size: 18px;
            }
            .toc-box ul {
                padding-left: 0;
                margin-bottom: 0;
            }
            .toc-box li {
                margin-bottom: 10px;
                font-size: 16px;
            }
            .toc-box a {
                text-decoration: none;
                color: #549ecd;
                font-weight: 600;
            }
            .toc-box a:hover {
                text-decoration: underline; /* Underline on hover */
                color: #1c83e1; /* Darker blue on hover */
            }
            </style>

            <div class="toc-box">
                <h4>–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ</h4>
                <ul>
                    <li><a href="#top-churn-states">–¢–æ–ø —à—Ç–∞—Ç–æ–≤ –ø–æ –æ—Ç—Ç–æ–∫—É</a></li>
                    <li><a href="#customer-distribution-by-state">–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —à—Ç–∞—Ç–∞–º</a></li>
                    <li><a href="#churn-by-state-and-category">–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞ –ø–æ —à—Ç–∞—Ç–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ç–æ–≤–∞—Ä–æ–≤</a></li>
                </ul>
            </div>
            """, unsafe_allow_html=True)

            col1, col2, col3 = st.columns(3)
            col1.metric("–í—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤", filtered_df['customer_unique_id'].nunique())
            col2.metric("–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞", 
                    f"{filtered_df['is_churned'].mean():.1%}")
            col3.metric("–°—Ä–µ–¥–Ω–∏–π RFM Score", 
                    f"{filtered_df['RFM_Score'].mean():.1f}")

            st.markdown("<a name='top-churn-states'></a>", unsafe_allow_html=True)
            # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ–ø —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –æ—Ç—Ç–æ–∫—É
            st.subheader("–¢–æ–ø —à—Ç–∞—Ç–æ–≤ –ø–æ –æ—Ç—Ç–æ–∫—É")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            churn_by_state = (filtered_df.groupby('customer_state')['is_churned']
                            .mean()
                            .sort_values(ascending=False)
                            .reset_index())
            churn_by_state.columns = ['–®—Ç–∞—Ç', '–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞']

            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ Plotly
            fig = px.bar(
                churn_by_state,
                x='–®—Ç–∞—Ç',
                y='–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞',
                color='–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞',
                color_continuous_scale='Blues',
                text='–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞'
            )

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞
            fig.update_traces(
                texttemplate='%{text:.1%}',
                textposition='outside'
            )
            
            fig.update_layout(
                xaxis_title='–®—Ç–∞—Ç',
                yaxis_title='',
                yaxis=dict(showticklabels=False, showgrid=False),
                yaxis_tickformat='.0%',
                coloraxis_showscale=False,
                hovermode='x',
                height=500
            )

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –≤ Streamlit
            st.plotly_chart(fig, use_container_width=True)

            st.markdown("<a name='customer-distribution-by-state'></a>", unsafe_allow_html=True)
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —à—Ç–∞—Ç–∞–º
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ —à—Ç–∞—Ç–∞–º")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            customers_by_state = (filtered_df['customer_state']
                                .value_counts()
                                .sort_values(ascending=False)
                                .reset_index())
            customers_by_state.columns = ['–®—Ç–∞—Ç', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤']

            # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
            fig2 = px.bar(
                customers_by_state,
                x='–®—Ç–∞—Ç',
                y='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤',
                color='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤',
                color_continuous_scale='Blues',
                text='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤',
            )

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–Ω–µ—à–Ω–µ–≥–æ –≤–∏–¥–∞
            fig2.update_traces(
                textposition='outside'
            )
            fig2.update_layout(
                xaxis_title='–®—Ç–∞—Ç',
                yaxis_title='',
                yaxis=dict(showticklabels=False, showgrid=False),
                coloraxis_showscale=False,
                hovermode='x',
                height=500
            )

            st.plotly_chart(fig2, use_container_width=True)

            merge6 = load_data()[2]

            heatmap_data = merge6.groupby(['customer_state', 'product_category_name'])['is_churned'].mean().reset_index()

            fig = px.density_heatmap(
            heatmap_data,
            x='customer_state',
            y='product_category_name',
            z='is_churned',
            color_continuous_scale='Blues',
            labels={'is_churned': '% –æ—Ç—Ç–æ–∫–∞'},
            width=1400,
            height=1000
            )

            fig.update_layout(
                xaxis_title="–®—Ç–∞—Ç",
                yaxis_title="–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–∞",
                hovermode='closest',

                font=dict(size=18),

                xaxis=dict(
                    title_font=dict(size=20),
                    tickfont=dict(size=16)
                ),
                yaxis=dict(
                    title_font=dict(size=20),
                    tickfont=dict(size=16)
                )
            )

            st.markdown("<a name='churn-by-state-and-category'></a>", unsafe_allow_html=True)
            st.subheader("–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞ –ø–æ —à—Ç–∞—Ç–∞–º –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ç–æ–≤–∞—Ä–æ–≤")
            st.plotly_chart(fig, use_container_width=True)

        # –í–∫–ª–∞–¥–∫–∞ 2: –î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –∫–∞—Ä—Ç–∞ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
        with tab2:
            st.subheader("RFM —Å–µ–≥–º–µ–Ω—Ç—ã")

            segt_map = {
                r'[1-2][1-2]': '–°–ø—è—â–∏–µ',
                r'[1-2][3-4]': '–í –∑–æ–Ω–µ —Ä–∏—Å–∫–∞',
                r'[1-2]5': '–ù–µ–ª—å–∑—è –ø–æ—Ç–µ—Ä—è—Ç—å',
                r'3[1-2]': '–ù–∞ –≥—Ä–∞–Ω–∏ —É—Ö–æ–¥–∞',
                r'33': '–¢—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è',
                r'[3-4][4-5]': '–õ–æ—è–ª—å–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã',
                r'41': '–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ',
                r'51': '–ù–æ–≤—ã–µ –∫–ª–∏–µ–Ω—Ç—ã',
                r'[4-5][2-3]': 'Potential loyalists',
                r'5[4-5]': '–ß–µ–º–ø–∏–æ–Ω—ã'
            }

            rfm = load_data()[1]
            rfm['Segment'] = rfm['R'].map(str) + rfm['F'].map(str)
            rfm['Segment'] = rfm['Segment'].replace(segt_map, regex=True)

            fig3 = rfm.groupby('Segment').agg({'customer_unique_id': lambda x: len(x)}).reset_index()

            fig3.rename(columns={'customer_unique_id': 'Count'}, inplace=True)
            fig3['percent'] = (fig3['Count'] / fig3['Count'].sum()) * 100
            fig3['percent'] = fig3['percent'].round(1)

            fig3['display_text'] = fig3['Segment'] + '<br>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: ' + fig3['Count'].astype(str) + '<br>' + fig3['percent'].astype(str) + '%'

            colors = [
                '#1a4b7d',
                '#3a6ea5',
                '#5d8fc7',
                '#89b0d9',
                '#b5d0e8',
                '#d9e6f2',
                '#e6eef7',
                '#f2f7fb'
            ]

            fig = px.treemap(fig3, path=['Segment'], values='Count',
                            width=800, height=800,
                            )

            fig.update_traces(text=fig3['display_text'],
                            textinfo='text',  
                            textposition='middle center',
                            textfont_size=18,
                            hovertemplate=(
                    "<b>%{label}</b><br>" +
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: %{value:,}<br>" +
                    "–ü—Ä–æ—Ü–µ–Ω—Ç: %{customdata[0]:.1f}%" +
                    "<extra></extra>"),
                customdata=fig3[['percent']])  

            fig.update_layout(
                treemapcolorway = colors, 
                margin=dict(t=50, l=25, r=25, b=25))

            st.plotly_chart(fig, use_container_width=True)

            st.subheader("–î–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞ –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

            # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
            customers_fix = pd.DataFrame()
            customers_fix["Recency"] = boxcox1p(rfm['Recency'], stats.boxcox_normmax(rfm['MonetaryValue'] + 1))
            customers_fix["Frequency"] = stats.boxcox(rfm['Frequency'])[0]
            customers_fix["Frequency"] = rfm['Frequency']
            customers_fix["MonetaryValue"] = stats.boxcox(rfm['MonetaryValue'])[0]

            scaler = StandardScaler()
            scaler.fit(customers_fix)
            customers_normalized = scaler.transform(customers_fix)

            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å 4 –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ (–≤—ã–±—Ä–∞–Ω–æ –ø–æ Elbow-–º–µ—Ç–æ–¥—É) –∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ rfm
            model = KMeans(n_clusters=4, random_state=42)
            model.fit(customers_normalized)
            rfm["Cluster"] = model.labels_

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç (Segment) –∏ —Ü–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ (Score).
            rfm['Score'] = '–°—Ç–∞–Ω–¥–∞—Ä—Ç'
            rfm.loc[rfm['RFM_Score']>5,'Score'] = '–ë—Ä–æ–Ω–∑–∞'
            rfm.loc[rfm['RFM_Score']>7,'Score'] = '–°–µ—Ä–µ–±—Ä–æ'
            rfm.loc[rfm['RFM_Score']>9,'Score'] = '–ó–æ–ª–æ—Ç–æ'
            rfm.loc[rfm['RFM_Score']>10,'Score'] = '–ü–ª–∞—Ç–∏–Ω–∞'

            # –¥–µ–Ω–¥—Ä–æ–≥—Ä–∞–º–º–∞
            fig5 = rfm.groupby(['Cluster', 'Segment', 'Score']).agg({'customer_unique_id': lambda x: len(x)}).reset_index()

            fig5.rename(columns={'customer_unique_id': 'Count'}, inplace=True)
            fig5['percent'] = (fig5['Count'] / fig5['Count'].sum()) * 100
            fig5['percent'] = fig5['percent'].round(1)

            colors = [
                '#3a6ea5',
                '#5d8fc7',
                '#89b0d9',
                '#b5d0e8',
                '#d9e6f2',
                '#e6eef7',
                '#f2f7fb'
            ]

            fig = px.treemap(fig5, path=['Cluster', 'Segment', 'Score'], values='Count',
                            width=800, height=1000,
                            hover_data={'Segment': True, 'Count': True, 'percent': True})

            fig.update_traces(textinfo='label+text+value+percent root', textfont_size=18)
            fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))

            fig.update_layout(
                treemapcolorway = colors,
                margin = dict(t=50, l=25, r=25, b=25))

            fig.data[0].textinfo = 'label+text+value+percent root'
            
            st.plotly_chart(fig, use_container_width=True)

        # –í–∫–ª–∞–¥–∫–∞ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        with tab3:
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RFM Score")
            fig1 = px.histogram(filtered_df, x='RFM_Score', color='is_churned',
                              nbins=20, barmode='overlay')
            st.plotly_chart(fig1, use_container_width=True)
            
            st.subheader("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            num_cols = ['payment_value', 'review_score', 'RFM_Score', 'is_churned']
            corr_matrix = filtered_df[num_cols].corr()
            fig2 = px.imshow(corr_matrix, text_auto=True,
                            width=800, height=600)
            st.plotly_chart(fig2, use_container_width=True)
        
        # –í–∫–ª–∞–¥–∫–∞ 4: –î–µ—Ç–∞–ª–∏ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
        with tab4:
            st.subheader("–î–µ—Ç–∞–ª–∏ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º")
            
            selected_customer = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∏–µ–Ω—Ç–∞:",
                options=filtered_df['customer_unique_id'].unique()
            )
            
            customer_data = filtered_df[filtered_df['customer_unique_id'] == selected_customer].iloc[0]
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("–†–µ–≥–∏–æ–Ω", customer_data['customer_state'])
                st.metric("RFM Score", customer_data['RFM_Score'])
                st.metric("–°—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫", f"{customer_data['payment_value']:.2f}")
            
            with col2:
                st.metric("–û—Ü–µ–Ω–∫–∞ –æ—Ç–∑—ã–≤–∞", customer_data['review_score'])
                is_churned = customer_data['is_churned'] == 1
                churn_label = "–î–∞" if is_churned else "–ù–µ—Ç"
                delta_text = "–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫" if is_churned else "–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫"
                st.metric("–°—Ç–∞—Ç—É—Å –æ—Ç—Ç–æ–∫–∞", 
                         churn_label,
                         delta=delta_text,
                         delta_color="inverse" if is_churned else "normal"
                         )
    
        # –í–∫–ª–∞–¥–∫–∞ 5: –ú–æ–¥–µ–ª–∏
        with tab5:
            st.header("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")
            
            X = filtered_df[numerical_features + categorical_features]
            y = filtered_df['is_churned']
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y)
            
            model_option = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
                ("Logistic Regression", "Decision Tree", "XGBoost")
            )
            
            if st.button("–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å"):
                with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                    if model_option == "Logistic Regression":
                        model = LogisticRegression(
                            class_weight='balanced',
                            max_iter=1000,
                            random_state=42
                        )
                    elif model_option == "Decision Tree":
                        model = DecisionTreeClassifier(
                            max_depth=5,
                            min_samples_leaf=50,
                            class_weight='balanced',
                            random_state=42
                        )
                    elif model_option == "XGBoost":
                        scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)
                        model = xgb.XGBClassifier(
                            objective='binary:logistic',
                            n_estimators=150,
                            max_depth=5,
                            learning_rate=0.1,
                            subsample=0.8,
                            colsample_bytree=0.8,
                            scale_pos_weight=scale_pos_weight,
                            random_state=42,
                            n_jobs=-1
                        )
                    
                    pipeline = train_model(model, X_train, y_train)
                    st.session_state.trained_pipeline = pipeline
                    st.session_state.model_option = model_option

                    y_pred, y_proba = predict_churn(pipeline, X_test)
                    
                    metrics = extended_classification_report(y_test, y_pred, y_proba, model_option)
                    
                    st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏")
                    col1, col2, col3 = st.columns(3)
                    col1.metric("ROC-AUC", f"{metrics['roc_auc']:.4f}")
                    col2.metric("F1 Score", f"{metrics['f1']:.4f}")
                    col3.metric("Accuracy", f"{metrics['accuracy']:.4f}")
                    
                    col4, col5, col6 = st.columns(3)
                    col4.metric("Sensitivity", f"{metrics['sensitivity']:.4f}")
                    col5.metric("Specificity", f"{metrics['specificity']:.4f}")
                    col6.metric("Precision", f"{metrics['precision']:.4f}")
                    
                    st.subheader("ROC-–∫—Ä–∏–≤–∞—è")
                    fpr, tpr, _ = roc_curve(y_test, y_proba)
                    fig = px.line(x=fpr, y=tpr, 
                                 labels={'x': 'False Positive Rate', 'y': 'True Positive Rate'},
                                 title=f'ROC Curve (AUC = {metrics["roc_auc"]:.2f})')
                    fig.add_shape(type='line', line=dict(dash='dash'),
                                x0=0, x1=1, y0=0, y1=1)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    st.subheader("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
                    cm = confusion_matrix(y_test, y_pred)
                    fig_cm = px.imshow(cm, 
                                       labels=dict(x="Predicted", y="Actual"),
                                       x=['Not Churned', 'Churned'],
                                       y=['Not Churned', 'Churned'],
                                       text_auto=True)
                    st.plotly_chart(fig_cm, use_container_width=True)
                    
                    st.success(f"–ú–æ–¥–µ–ª—å {model_option} —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")

            if 'trained_pipeline' in st.session_state:
                st.subheader("–°–∫–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
                pipeline_to_save = st.session_state.trained_pipeline
                model_option_to_save = st.session_state.model_option
                model_filename = f'{model_option_to_save.replace(" ", "_").lower()}_model.pkl'
                
                from io import BytesIO
                model_bytes = BytesIO()
                joblib.dump(pipeline_to_save, model_bytes)
                model_bytes.seek(0)

                st.download_button(
                    label=f"–°–∫–∞—á–∞—Ç—å {model_option_to_save.replace(" ", "_")}.pkl",
                    data=model_bytes,
                    file_name=model_filename,
                    mime="application/octet-stream"
                )

            st.subheader("–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞")
            customer_id_predict = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞:",
                options=filtered_df['customer_unique_id'].unique(),
                key="customer_predict_select"
            )
            
            if st.button("–°–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑"):
                if 'trained_pipeline' not in st.session_state:
                    st.error("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–æ–π –≤–∫–ª–∞–¥–∫–µ.")
                else:
                    try:
                        pipeline = st.session_state.trained_pipeline
                        current_model_option = st.session_state.model_option
                        
                        customer_data = filtered_df[filtered_df['customer_unique_id'] == customer_id_predict].iloc[0:1]
                        X_customer = customer_data[numerical_features + categorical_features]
                        
                        pred, proba = predict_churn(pipeline, X_customer)
                        
                        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞")
                        col1, col2 = st.columns(2)
                        col1.metric("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞", f"{proba[0]:.2%}")

                        is_high_risk = pred[0] == 1
                        prognosis_label = "–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞" if is_high_risk else "–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞"
                        delta_display_text = "‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ" if is_high_risk else "‚úÖ –ù–æ—Ä–º–∞"

                        col2.metric("–ü—Ä–æ–≥–Ω–æ–∑",
                                   prognosis_label,
                                   delta=delta_display_text,
                                   delta_color="inverse" if is_high_risk else "normal"
                                   )

                        if current_model_option in ["Decision Tree", "XGBoost"]:
                            st.subheader("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                            try:
                                classifier = pipeline.named_steps['classifier']
                                
                                preprocessor_step = pipeline.named_steps['preprocessor']
                                
                                ohe_feature_names = preprocessor_step.transformers_[1][1]\
                                    .get_feature_names_out(categorical_features)
                                
                                feature_names = numerical_features + list(ohe_feature_names)
                                
                                importances = classifier.feature_importances_
                                
                                importance_df = pd.DataFrame({
                                    'feature': feature_names,
                                    'importance': importances
                                }).sort_values(by='importance', ascending=False)
                                
                                top_n = 15
                                fig_importance = px.bar(
                                    importance_df.head(top_n),
                                    x='importance',
                                    y='feature',
                                    orientation='h',
                                    title=f'–¢–æ–ø-{top_n} –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {current_model_option}'
                                )
                                st.plotly_chart(fig_importance, use_container_width=True)
                                
                            except AttributeError:
                                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {current_model_option}.")
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")

                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏: {str(e)}")

            # –í–∫–ª–∞–¥–∫–∞ 6: –ö–∞—Ä—Ç–∞
            with tab6:
                geo    = load_geo()
                orders = load_orders()

                T, PX = 0.05, 1

                # ‚îÄ‚îÄ‚îÄ HoloViews ‚Üí Bokeh opts (FULL‚ÄëWIDTH, RESPONSIVE) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                hv.extension("bokeh", logo=False)

                gv.opts.defaults(
                    gv.opts.Overlay(
                        height   = 700,
                        toolbar  = "above",
                        xaxis    = None,
                        yaxis    = None,
                        responsive=True               # —ç—Ç–æ –≤–∞–∂–Ω–æ
                    ),
                    gv.opts.QuadMesh(
                        tools      = ["hover"],
                        colorbar   = True,
                        alpha      = 0,
                        hover_alpha= 0.2,
                    ),
                )

                # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

                def make_map(df, col, title, cmap):
                    """
                    returns a holoviews overlay (bokeh renderable) showing df[col]
                    """
                    url = "https://server.arcgisonline.com/ArcGIS/rest/services/" \
                        "Canvas/World_Dark_Gray_Base/MapServer/tile/{Z}/{Y}/{X}.png"
                    tiles  = gv.WMTS(WMTSTileSource(url=url))
                    pts    = hv.Points(gv.Dataset(df, kdims=["x", "y"], vdims=[col]))
                    shade  = datashade(pts, element_type=gv.Image, aggregator=ds.mean(col), cmap=cmap)
                    spread = dynspread(shade, threshold=T, max_px=PX)
                    hover  = hv.util.Dynamic(
                        rasterize(pts, aggregator=ds.mean(col), width=50, height=25, streams=[RangeXY]),
                        operation=hv.QuadMesh,
                    ).opts(cmap=cmap)
                    return (tiles * spread * hover).relabel(title)

                def clip_outliers(df):
                    return df[
                        df.x.between(df.x.quantile(0.001), df.x.quantile(0.999)) &
                        df.y.between(df.y.quantile(0.001), df.y.quantile(0.999))
                    ]

                def build_revenue_df(geo, orders):
                    geo3 = geo.set_index("geolocation_zip_code_prefix_3_digits").copy()
                    gp   = orders.groupby("customer_zip_code_prefix_3_digits")["price"].sum()
                    df   = geo3.join(gp)
                    df["revenue"] = df["price"].fillna(0) / 1_000          # k‚ÄØR$
                    return df

                def build_avg_ticket_df(geo, orders):
                    tickets = (
                        orders.groupby("order_id")
                            .agg({"price": "sum",
                                    "customer_zip_code_prefix_3_digits": "first"})
                    )
                    gp = tickets.groupby("customer_zip_code_prefix_3_digits")["price"].mean()
                    geo3 = geo.set_index("geolocation_zip_code_prefix_3_digits").copy()
                    df = geo3.join(gp)
                    df["avg_ticket"] = df["price"]
                    return df

                @st.cache_data(show_spinner=True)
                def build_freight_ratio_df(geo, orders):
                    tmp = (
                        orders.groupby("order_id")
                            .agg({"price": "sum",
                                    "freight_value": "sum",
                                    "customer_zip_code_prefix_3_digits": "first"})
                    )

                    tmp = tmp[tmp["price"] > 0]
                    tmp["freight_ratio"] = tmp["freight_value"] / tmp["price"]

                    gp = tmp.groupby("customer_zip_code_prefix_3_digits")["freight_ratio"].mean()

                    geo3 = geo.set_index("geolocation_zip_code_prefix_3_digits").copy()
                    return geo3.join(gp)

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ sidebar selectors ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                st.header("üìç –§–∏–ª—å—Ç—Ä –∫–∞—Ä—Ç—ã")

                states = ["<–≤—Å—è –ë—Ä–∞–∑–∏–ª–∏—è>"] + sorted(geo["geolocation_state"].unique())
                state  = st.selectbox("–®—Ç–∞—Ç (UF)", states, index=0)

                if state == "<–≤—Å—è –ë—Ä–∞–∑–∏–ª–∏—è>":
                    sub_geo    = geo.copy()
                    sub_orders = orders.copy()
                    city       = "<–≤—Å–µ>"
                else:
                    sub_geo    = geo[geo["geolocation_state"] == state]
                    sub_orders = orders[orders["customer_state"] == state]
                    cities     = ["<–≤—Å–µ>"] + sorted(sub_geo["geolocation_city"].str.lower().unique())
                    city       = st.selectbox("–ì–æ—Ä–æ–¥", cities, index=0)
                    if city != "<–≤—Å–µ>":
                        sub_geo    = sub_geo[sub_geo["geolocation_city"].str.lower() == city]
                        sub_orders = sub_orders[sub_orders["customer_city"].str.lower() == city]

                sub_geo = clip_outliers(sub_geo)

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ revenue map ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                st.subheader("üí∞ –î–æ—Ö–æ–¥ –æ—Ç –∑–∞–∫–∞–∑–æ–≤ (—Ç—ã—Å.¬†R$)")

                revenue_df = build_revenue_df(sub_geo, sub_orders)

                col1, col2 = st.columns(2)

                with col1:
                    st.metric("ZIP‚Äë–ø—Ä–µ—Ñ–∏–∫—Å–æ–≤", len(revenue_df))
                with col2:
                    st.metric("–í—Å–µ–≥–æ (k¬†R$)", int(revenue_df["revenue"].sum()))

                with st.spinner("üñºÔ∏è —Ä–µ–Ω–¥–µ—Ä –¥–æ—Ö–æ–¥–∞ ‚Ä¶"):
                    fig_rev = hv.render(make_map(revenue_df, "revenue",
                                                "Orders Revenue (k¬†R$)", fire),
                                        backend="bokeh")
                fig_rev.sizing_mode = "stretch_width"
                streamlit_bokeh(fig_rev, use_container_width=True)

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ avg ticket map ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                st.subheader("üéüÔ∏è –°—Ä–µ–¥–Ω–∏–π —á–µ–∫ –∑–∞–∫–∞–∑–∞ (R$)")

                avg_df = build_avg_ticket_df(sub_geo, sub_orders)

                st.metric("–°—Ä–µ–¥–Ω–∏–π —á–µ–∫ (R$)", f"{avg_df['avg_ticket'].mean():.2f}")

                with st.spinner("üñºÔ∏è —Ä–µ–Ω–¥–µ—Ä —Å—Ä–µ–¥–Ω–µ–≥–æ —á–µ–∫–∞ ‚Ä¶"):
                    fig_avg = hv.render(make_map(avg_df, "avg_ticket",
                                                "Orders Average Ticket (R$)", bgy),
                                        backend="bokeh")
                fig_avg.sizing_mode = "stretch_width"
                streamlit_bokeh(fig_avg, use_container_width=True)

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ freight‚Äëratio map ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                freight_df = build_freight_ratio_df(sub_geo, sub_orders)

                st.subheader("üöö –°—Ä–µ–¥–Ω–µ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Å—Ç–∞–≤–∫–∏ –∫ —Ü–µ–Ω–µ")

                st.metric("–°—Ä–µ–¥–Ω–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Å—Ç–∞–≤–∫–∞/—á–µ–∫", f"{freight_df['freight_ratio'].mean():.2%}")

                with st.spinner("üñºÔ∏è —Ä–µ–Ω–¥–µ—Ä freight-ratio map ‚Ä¶"):
                    fig_freight = hv.render(
                        make_map(freight_df, "freight_ratio",
                                "Orders Average Freight Ratio", kr),
                        backend="bokeh"
                    )
                fig_freight.sizing_mode = "stretch_width"
                streamlit_bokeh(fig_freight, use_container_width=True)

            with tab7:
                import streamlit.components.v1 as components

                PBI_EMBED_URL = "https://app.powerbi.com/reportEmbed?reportId=67889dcb-3cc3-4b1c-9a22-14472a156917&autoAuth=true&ctid=dfe014b9-885d-4e4a-8eb4-597464b165c5"

                components.html(f"""
                    <iframe 
                        width="100%" 
                        height="800" 
                        src="{PBI_EMBED_URL}" 
                        frameborder="0" 
                        allowFullScreen="true">
                    </iframe>
                """, height=800)


    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        st.stop()

if __name__ == "__main__":
    main()