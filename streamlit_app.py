# ==============================================
# –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
# ==============================================

# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
import streamlit as st  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
import pandas as pd  # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–∞–±–ª–∏—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
import numpy as np  # –î–ª—è —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
import matplotlib.pyplot as plt  # –î–ª—è –±–∞–∑–æ–≤–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
import seaborn as sns  # –î–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
import plotly.express as px  # –î–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –≥–µ–æ–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
import holoviews as hv  # –î–ª—è —Å–ª–æ–∂–Ω–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
import geoviews as gv  # –î–ª—è –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ä—Ç
import datashader as ds  # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
from datashader.utils import lnglat_to_meters as webm  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
from holoviews.operation.datashader import datashade, dynspread, rasterize  # –û–ø–µ—Ä–∞—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
from holoviews.streams import RangeXY  # –î–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
from colorcet import rainbow, fire  # –¶–≤–µ—Ç–æ–≤—ã–µ –ø–∞–ª–∏—Ç—Ä—ã
from bokeh.models import WMTSTileSource  # –î–ª—è —Ñ–æ–Ω–æ–≤—ã—Ö –∫–∞—Ä—Ç
from streamlit_bokeh import streamlit_bokeh  # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Bokeh —Å Streamlit

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
import joblib  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π
from datetime import datetime, timedelta  # –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞–º–∏
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
from sklearn.compose import ColumnTransformer  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤
from sklearn.pipeline import Pipeline  # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤
from sklearn.metrics import (  # –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
    classification_report, roc_auc_score, confusion_matrix, 
    recall_score, precision_score, f1_score, accuracy_score, roc_curve
)
from sklearn.model_selection import train_test_split  # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
from sklearn.linear_model import LogisticRegression  # –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
from sklearn.tree import DecisionTreeClassifier  # –†–µ—à–∞—é—â–∏–µ –¥–µ—Ä–µ–≤—å—è
import xgboost as xgb  # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥
from scipy import stats  # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏

# ==============================================
# –ù–ê–°–¢–†–û–ô–ö–ê –°–¢–†–ê–ù–ò–¶–´ STREAMLIT
# ==============================================
st.set_page_config(
    page_title="Customer Churn Analytics Dashboard",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==============================================
# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ó–ê–ì–†–£–ó–ö–ò –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–•
# ==============================================

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
@st.cache_data  # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
def load_and_preprocess_data():
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö CSV-—Ñ–∞–π–ª–æ–≤
    customers = pd.read_csv('customers.csv')
    orders = pd.read_csv('orders.csv')
    order_payments = pd.read_csv('order_payments.csv')
    order_reviews = pd.read_csv('order_reviews.csv')
    products = pd.read_csv('products.csv')
    product_category = pd.read_csv('product_category_name_translation.csv')
    sellers = pd.read_csv('sellers.csv')
    order_items = pd.read_csv('orders_items.csv')

    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ 'Unnamed: 0' –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    for df in [order_payments, order_reviews, product_category, sellers, order_items]:
        if 'Unnamed: 0' in df.columns:
            df.drop('Unnamed: 0', axis=1, inplace=True)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç –≤ orders
    date_cols = [
        'order_purchase_timestamp', 
        'order_approved_at',
        'order_delivered_carrier_date', 
        'order_delivered_customer_date',
        'order_estimated_delivery_date'
    ]

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ –≤ datetime —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –±–µ–∑ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    for col in date_cols:
        orders[col] = pd.to_datetime(orders[col], format='%Y-%m-%d %H:%M:%S', errors='coerce')
        # –õ–æ–≥–≥–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞, –Ω–æ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤ UI
        invalid_count = orders[col].isnull().sum()

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤
    order_reviews = pd.read_csv('order_reviews.csv')
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –æ—Ç–∑—ã–≤–æ–≤
    order_reviews['review_creation_date'] = pd.to_datetime(
        order_reviews['review_creation_date'], 
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')

    order_reviews['review_answer_timestamp'] = pd.to_datetime(
        order_reviews['review_answer_timestamp'],
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç –≤ order_items
    order_items['shipping_limit_date'] = pd.to_datetime(
        order_items['shipping_limit_date'],
        format='%Y-%m-%d %H:%M:%S', 
        errors='coerce')

    order_items['shipping_limit_date.1'] = pd.to_datetime(
        order_items['shipping_limit_date.1'],
        format='%Y-%m-%d %H:%M:%S',
        errors='coerce')
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ orders –∏ customers
    orders = pd.merge(
        orders, 
        customers[['customer_id', 'customer_unique_id', 'customer_city', 'customer_state']], 
        on='customer_id', 
        how='inner'
    )
    orders.drop('customer_id', axis=1, inplace=True)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∑–∞–∫–∞–∑–æ–≤ –∫–ª–∏–µ–Ω—Ç–∞
    orders['quantity'] = orders.groupby('customer_unique_id')['order_id'].transform('nunique')

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Ç–∑—ã–≤–∞–º–∏
    merge1 = pd.merge(
        orders,
        order_reviews[['order_id', 'review_creation_date', 'review_score']],
        on='order_id',
        how='left'
    )
    
    # –£–¥–∞–ª–µ–Ω–∏–µ NA –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
    merge1.dropna(subset=['review_creation_date', 'review_score'], inplace=True)
    merge1['review_creation_date'] = pd.to_datetime(merge1['review_creation_date'])

    # –†–∞—Å—á–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç—Ç–æ–∫–∞
    analysis_date = merge1['order_purchase_timestamp'].max() + timedelta(days=1)
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–ª–∏–µ–Ω—Ç—É –∏ –¥–∞—Ç–µ –∑–∞–∫–∞–∑–∞
    df_sorted = merge1.sort_values(['customer_unique_id', 'order_purchase_timestamp'])
    
    # –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É –∑–∞–∫–∞–∑–∞–º–∏ (–∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
    df_sorted['time_diff_days'] = df_sorted.groupby('customer_unique_id')['order_purchase_timestamp'].diff().dt.days
    
    # –ú–µ–¥–∏–∞–Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –∑–∞–∫–∞–∑–∞–º–∏
    customer_intervals = df_sorted.groupby('customer_unique_id')['time_diff_days'].median().reset_index()
    customer_intervals.rename(columns={'time_diff_days': 'median_interval_days'}, inplace=True)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
    customer_stats = merge1.groupby('customer_unique_id').agg(
        last_order_date=('order_purchase_timestamp', 'max'),
        order_count=('order_id', 'nunique')
    ).reset_index()
    
    # –î–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–∫–∞–∑–∞
    customer_stats['days_since_last_order'] = (analysis_date - customer_stats['last_order_date']).dt.days
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
    customer_features = pd.merge(customer_stats, customer_intervals, on='customer_unique_id', how='left')
    
    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥ –æ—Ç—Ç–æ–∫–∞
    churn_threshold_multiplier = 2.0
    default_churn_days = customer_features['days_since_last_order'].mean()
    
    def calculate_dynamic_churn(row):
        # –ï—Å–ª–∏ —É –∫–ª–∏–µ–Ω—Ç–∞ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∑–∞–∫–∞–∑, —Å—á–∏—Ç–∞–µ—Ç—Å—è —É—à–µ–¥—à–∏–º, –µ—Å–ª–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–∫–∞–∑–∞ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ default_churn_days
        if row['order_count'] == 1:
            return 1 if row['days_since_last_order'] > default_churn_days else 0
        # –ï—Å–ª–∏ –º–µ–¥–∏–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∑–∞–∫–∞–∑–∞–º–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω (–º–µ–Ω—å—à–µ/—Ä–∞–≤–µ–Ω –Ω—É–ª—é), —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ—Ç—Ç–æ–∫–∞
        elif pd.isna(row['median_interval_days']) or row['median_interval_days'] <= 0:
            return 1 if row['days_since_last_order'] > default_churn_days else 0
        else:
            # –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –æ—Ç—Ç–æ–∫–∞
            personalized_threshold = row['median_interval_days'] * churn_threshold_multiplier
            # –ï—Å–ª–∏ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–∫–∞–∑–∞ –ø—Ä–æ—à–ª–æ –±–æ–ª—å—à–µ, —á–µ–º —ç—Ç–æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ ‚Äî –∫–ª–∏–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è —É—à–µ–¥—à–∏–º
            return 1 if row['days_since_last_order'] > personalized_threshold else 0
    
    customer_features['is_churned'] = customer_features.apply(calculate_dynamic_churn, axis=1)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    merge1 = pd.merge(
        merge1,
        customer_features[['customer_unique_id', 'is_churned', 'days_since_last_order', 'median_interval_days']],
        on='customer_unique_id',
        how='left'
    )

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ products
    products = pd.merge(
        products, 
        product_category, 
        on='product_category_name', 
        how='left'
    )
    products.drop('product_category_name', axis=1, inplace=True)
    products = products.rename(columns={'product_category_name_english': 'product_category_name'})
    products['product_category_name'] = products['product_category_name'].str.strip().str.lower()
    products.dropna(subset=['product_category_name'], inplace=True)
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    category_corrections = {
        'home_confort': 'home_comfort',
        'home_appliances_2': 'home_appliances',
    }
    products['product_category_name'] = products['product_category_name'].replace(category_corrections)

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Ç–æ–≤–∞—Ä–∞–º–∏ –∏ –ø—Ä–æ–¥–∞–≤—Ü–∞–º–∏
    merge2 = pd.merge(
        products,
        order_items[['order_id', 'product_id', 'seller_id', 'price']],
        on='product_id',
        how='left'
    ).drop_duplicates()

    merge3 = pd.merge(
        merge2,
        sellers[['seller_id', 'seller_city', 'seller_state']],
        on='seller_id',
        how='left'
    )

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
    merge4 = pd.merge(
        merge1,
        merge3,
        on='order_id',
        how='left'
    )

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–ª–∞—Ç–µ–∂–µ–π
    merge5 = pd.merge(
        merge4,
        order_payments[['order_id', 'payment_type', 'payment_installments', 'payment_value']],
        on='order_id',
        how='left'
    ).drop_duplicates()

    # –£–¥–∞–ª–µ–Ω–∏–µ NA –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–∞—Ö
    merge5 = merge5.dropna(subset=['product_id', 'payment_type'])

    # –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    merge5['approval_delay'] = (merge5['order_approved_at'] - merge5['order_purchase_timestamp']).dt.total_seconds()
    merge5['delivery_time_to_customer_days'] = (merge5['order_delivered_customer_date'] - merge5['order_purchase_timestamp']).dt.days
    merge5['delivery_delay'] = (merge5['order_delivered_customer_date'] - merge5['order_estimated_delivery_date']).dt.days
    merge5['estimated_delivery_time'] = (merge5['order_estimated_delivery_date'] - merge5['order_purchase_timestamp']).dt.days

    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ª–æ–≥–∏—á–Ω—ã—Ö —Å—Ç—Ä–æ–∫ (–ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫)
    merge5 = merge5[
        (merge5['order_approved_at'] < merge5['order_delivered_customer_date']) &
        (merge5['order_approved_at'] < merge5['order_delivered_carrier_date']) &
        (merge5['order_delivered_carrier_date'] < merge5['order_delivered_customer_date'])
    ]

    # RFM –∞–Ω–∞–ª–∏–∑ (Recency, Frequency, Monetary)
    rfm = merge5.groupby('customer_unique_id').agg({
        'order_purchase_timestamp': 'max',
        'order_id': 'nunique',
        'payment_value': 'sum'
    }).reset_index()
    
    rfm.columns = ['customer_unique_id', 'last_order', 'Frequency', 'MonetaryValue']
    rfm['Recency'] = (analysis_date - rfm['last_order']).dt.days
    rfm.drop('last_order', axis=1, inplace=True)
    
    # –†–∞—Å—á–µ—Ç RFM Scores
    quantiles = rfm[['Recency', 'Frequency', 'MonetaryValue']].quantile(q=[0.2, 0.4, 0.6, 0.8]).to_dict()
    
    def RScore(x, p, d):
        if x <= d[p][0.2]: return 5
        elif x <= d[p][0.4]: return 4
        elif x <= d[p][0.6]: return 3
        elif x <= d[p][0.8]: return 2
        else: return 1
    
    def FMScore(x, p, d):
        if x <= d[p][0.2]: return 1
        elif x <= d[p][0.4]: return 2
        elif x <= d[p][0.6]: return 3
        elif x <= d[p][0.8]: return 4
        else: return 5
    
    rfm['R'] = rfm['Recency'].apply(RScore, args=('Recency', quantiles))
    rfm['F'] = rfm['Frequency'].apply(FMScore, args=('Frequency', quantiles))
    rfm['M'] = rfm['MonetaryValue'].apply(FMScore, args=('MonetaryValue', quantiles))
    rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ RFM —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    merge6 = pd.merge(
        merge5,
        rfm[['customer_unique_id', 'Frequency', 'MonetaryValue', 'Recency', 'R', 'F', 'M', 'RFM_Score']],
        on='customer_unique_id',
        how='left'
    )

    # –í—ã–±–æ—Ä —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
    features = ['customer_unique_id', 'customer_city', 'customer_state', 'order_status', 'payment_type',
        'payment_installments', 'payment_value', 'review_score', 'price',
        'product_weight_g', 'seller_city', 'seller_state', 'product_category_name',
        'approval_delay', 'delivery_time_to_customer_days', 'estimated_delivery_time',
        'delivery_delay', 'RFM_Score', 'is_churned'
    ]
    merge6.dropna(subset=features, inplace=True)
    
    return merge6[features], rfm

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
@st.cache_data(show_spinner=True)
def load_and_prepare_geo():
    geo = pd.read_csv("geolocation.csv", dtype={'geolocation_zip_code_prefix': str})

    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –ø–æ—á—Ç–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
    for n in range(1, 5):
        geo[f'geolocation_zip_code_prefix_{n}_digits'] = geo['geolocation_zip_code_prefix'].str[:n]

    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ë—Ä–∞–∑–∏–ª–∏–∏
    geo = geo.query(
        "geolocation_lat <= 5.27438888 and geolocation_lng >= -73.98283055 "
        "and geolocation_lat >= -33.75116944 and geolocation_lng <= -34.79314722"
    )

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ –º–µ—Ç—Ä—ã
    geo['x'], geo['y'] = webm(geo.geolocation_lng, geo.geolocation_lat)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –≤ —á–∏—Å–ª–∞
    int_cols = [c for c in geo.columns if 'prefix' in c]
    geo[int_cols] = geo[int_cols].astype(int)

    return geo

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ –∑–∞–∫–∞–∑–∞—Ö
@st.cache_data(show_spinner=True)
def load_orders():
    orders_df = pd.read_csv("orders.csv")
    order_items = pd.read_csv("orders_items.csv")
    order_reviews = pd.read_csv("order_reviews.csv")
    customers = pd.read_csv("customers.csv", dtype={'customer_zip_code_prefix': str})

    # –°–æ–∑–¥–∞–Ω–∏–µ 3-–∑–Ω–∞—á–Ω–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –ø–æ—á—Ç–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
    customers['customer_zip_code_prefix_3_digits'] = (
        customers['customer_zip_code_prefix'].str[:3].astype(int)
    )

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü –≤ –æ–¥–Ω—É
    orders = (
        orders_df
        .merge(order_items, on='order_id')
        .merge(customers, on='customer_id')
        .merge(order_reviews, on='order_id')
    )

    return orders

# ==============================================
# –ù–ê–°–¢–†–û–ô–ö–ò –ú–û–î–ï–õ–ï–ô –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
# ==============================================

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
numerical_features = ['payment_value','review_score','price','product_weight_g',
                  'approval_delay', 'RFM_Score']
categorical_features = ['customer_city','customer_state','order_status','payment_type',
                      'payment_installments','seller_city','seller_state','product_category_name']

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç–∏ (True Negative Rate)
def calculate_specificity(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
def extended_classification_report(y_true, y_pred, y_proba, model_name):
    metrics = {
        'specificity': calculate_specificity(y_true, y_pred),
        'sensitivity': recall_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'roc_auc': roc_auc_score(y_true, y_proba),
        'accuracy': accuracy_score(y_true, y_pred)
    }
    return metrics

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    ])

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
def train_model(model, X_train, y_train):
    pipeline = Pipeline([
        ('preprocessor', preprocessor),  # –≠—Ç–∞–ø –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        ('classifier', model)  # –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    ])
    pipeline.fit(X_train, y_train)
    return pipeline

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞
def predict_churn(model, X):
    y_pred = model.predict(X)  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã
    y_proba = model.predict_proba(X)[:, 1]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
    return y_pred, y_proba

# –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
@st.cache_data
def load_data():
    return load_and_preprocess_data()

# ==============================================
# –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ó–ê–ü–£–°–ö–ê –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ==============================================
def main():
    st.title("üìä –ê–Ω–∞–ª–∏–∑ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—à–∏–±–æ–∫
    try:
        df = load_data()[0]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ['customer_state', 'payment_value', 'review_score', 
                          'RFM_Score', 'is_churned']
        missing_cols = [col for col in required_columns if col not in df.columns]
        
        if missing_cols:
            st.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}")
            st.stop()
        
        # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
        st.sidebar.header("üîç –§–∏–ª—å—Ç—Ä—ã")
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–≥–∏–æ–Ω—É (—à—Ç–∞—Ç—É)
        selected_states = st.sidebar.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–≥–∏–æ–Ω—ã:",
            options=df['customer_state'].unique(),
            default=df['customer_state'].unique()
        )
        
        # –§–∏–ª—å—Ç—Ä –ø–æ RFM Score
        rfm_range = st.sidebar.slider(
            "–î–∏–∞–ø–∞–∑–æ–Ω RFM Score:",
            min_value=int(df['RFM_Score'].min()),
            max_value=int(df['RFM_Score'].max()),
            value=(int(df['RFM_Score'].min()), int(df['RFM_Score'].max()))
        )
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ –¥–∞–Ω–Ω—ã–º
        filtered_df = df[
            (df['customer_state'].isin(selected_states)) &
            (df['RFM_Score'].between(rfm_range[0], rfm_range[1]))
        ]
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∫–ª–∞–¥–æ–∫ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["üìà –û–±–∑–æ—Ä", "üå≥ –î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –∫–∞—Ä—Ç–∞", "üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è", "üîç –î–µ—Ç–∞–ª–∏", "ü§ñ –ú–æ–¥–µ–ª–∏", "üó∫Ô∏è –ö–∞—Ä—Ç–∞"])
        
        # –í–∫–ª–∞–¥–∫–∞ 1: –û–±–∑–æ—Ä
        with tab1:
            # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
            col1, col2, col3 = st.columns(3)
            col1.metric("–í—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤", filtered_df['customer_unique_id'].nunique())
            col2.metric("–£—Ä–æ–≤–µ–Ω—å –æ—Ç—Ç–æ–∫–∞", 
                       f"{filtered_df['is_churned'].mean():.1%}",
                       help="–î–æ–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å is_churned=1")
            col3.metric("–°—Ä–µ–¥–Ω–∏–π RFM Score", 
                       f"{filtered_df['RFM_Score'].mean():.1f}")
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç—Ç–æ–∫–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
            st.subheader("–¢–æ–ø —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –æ—Ç—Ç–æ–∫—É")
            churn_by_state = filtered_df.groupby('customer_state')['is_churned'].mean().sort_values(ascending=False)
            st.bar_chart(churn_by_state)

        # –í–∫–ª–∞–¥–∫–∞ 2: –î—Ä–µ–≤–æ–≤–∏–¥–Ω–∞—è –∫–∞—Ä—Ç–∞ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º
        with tab2:
            # –°–æ–∑–¥–∞–µ–º –ø–æ–Ω—è—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                segt_map = {
                    r'[1-2][1-2]': '–°–ø—è—â–∏–µ', #Hibernating
                    r'[1-2][3-4]': '–í –∑–æ–Ω–µ —Ä–∏—Å–∫–∞', #At risk
                    r'[1-2]5': '–ù–µ–ª—å–∑—è –ø–æ—Ç–µ—Ä—è—Ç—å', #Can\'t lose them
                    r'3[1-2]': '–ù–∞ –≥—Ä–∞–Ω–∏ —É—Ö–æ–¥–∞', #About to sleep
                    r'33': '–¢—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è', #Need attention
                    r'[3-4][4-5]': '–õ–æ—è–ª—å–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã', #Loyal customers
                    r'41': '–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ', #Promising
                    r'51': '–ù–æ–≤—ã–µ –∫–ª–∏–µ–Ω—Ç—ã', #New customers
                    r'[4-5][2-3]': 'Potential loyalists', #Potential loyalists
                    r'5[4-5]': '–ß–µ–º–ø–∏–æ–Ω—ã' #Champions
                }

                rfm = load_data()[1]
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–ª RFM_Score.
                # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —á–∏—Ç–∞–µ–º—ã–µ –º–µ—Ç–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞–º
                rfm['Segment'] = rfm['R'].map(str) + rfm['F'].map(str)
                rfm['Segment'] = rfm['Segment'].replace(segt_map, regex=True)

                # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞.
                fig3 = rfm.groupby('Segment').agg({'customer_unique_id': lambda x: len(x)}).reset_index()

                fig3.rename(columns={'customer_unique_id': 'Count'}, inplace=True)
                fig3['percent'] = (fig3['Count'] / fig3['Count'].sum()) * 100
                fig3['percent'] = fig3['percent'].round(1)

                # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–∞–∑–≤–∞–Ω–∏–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –ø—Ä–æ—Ü–µ–Ω—Ç)
                fig3['display_text'] = fig3['Segment'] + '<br>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: ' + fig3['Count'].astype(str) + '<br>' + fig3['percent'].astype(str) + '%'

                # –°—Ç—Ä–æ–∏–º –¥—Ä–µ–≤–æ–≤–∏–¥–Ω—É—é –∫–∞—Ä—Ç—É (Treemap) –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º.
                #colors=['#83af70','#9fbf8f','#bad0af','#d5e0cf','#f1f1f1','#f1d4d4','#f0b8b8','#ec9c9d'] #green
                colors = [
                    '#1a4b7d',  # –ì–ª—É–±–æ–∫–∏–π –ø—Ä–∏–≥–ª—É—à–µ–Ω–Ω—ã–π —Å–∏–Ω–∏–π (–∫–∞–∫ –≤ RdBu –¥–ª—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
                    '#3a6ea5',  # –ù–∞—Å—ã—â–µ–Ω–Ω—ã–π —Å–∏–Ω–∏–π
                    '#5d8fc7',  # –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Å–∏–Ω–∏–π
                    '#89b0d9',  # –°–≤–µ—Ç–ª–æ-—Å–∏–Ω–∏–π
                    '#b5d0e8',  # –û—á–µ–Ω—å —Å–≤–µ—Ç–ª—ã–π —Å–∏–Ω–∏–π
                    '#d9e6f2',  # –ü–æ—á—Ç–∏ –±–µ–ª—ã–π —Å —Å–∏–Ω–∏–º –æ—Ç—Ç–µ–Ω–∫–æ–º
                    '#e6eef7',  # –ï—â–µ —Å–≤–µ—Ç–ª–µ–µ
                    '#f2f7fb'   # –ü–æ—á—Ç–∏ –±–µ–ª—ã–π
                ]

                #import plotly.express as px

                fig = px.treemap(fig3, path=['Segment'], values='Count',
                                width=800, height=400,
                                title="RFM —Å–µ–≥–º–µ–Ω—Ç—ã")

                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
                fig.update_traces(text=fig3['display_text'],
                                textinfo='text',  
                                textposition='middle center',
                                textfont_size=14,
                                hovertemplate=(
                        "<b>%{label}</b><br>" +
                        "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: %{value:,}<br>" +
                        "–ü—Ä–æ—Ü–µ–Ω—Ç: %{customdata[0]:.1f}%" +
                        "<extra></extra>"),
                    customdata=fig3[['percent']])  


                fig.update_layout(
                    treemapcolorway = colors, 
                    margin=dict(t=50, l=25, r=25, b=25))

                st.plotly_chart(fig, use_container_width=True)

        # –í–∫–ª–∞–¥–∫–∞ 3: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        with tab3:
            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è RFM Score
            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ RFM Score")
            fig1 = px.histogram(filtered_df, x='RFM_Score', color='is_churned',
                              nbins=20, barmode='overlay')
            st.plotly_chart(fig1, use_container_width=True)
            
            # –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            st.subheader("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            num_cols = ['payment_value', 'review_score', 'RFM_Score', 'is_churned']
            corr_matrix = filtered_df[num_cols].corr()
            fig2 = px.imshow(corr_matrix, text_auto=True)
            st.plotly_chart(fig2, use_container_width=True)
        
        # –í–∫–ª–∞–¥–∫–∞ 4: –î–µ—Ç–∞–ª–∏ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º
        with tab4:
            st.subheader("–î–µ—Ç–∞–ª–∏ –ø–æ –∫–ª–∏–µ–Ω—Ç–∞–º")
            
            # –í—ã–±–æ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            selected_customer = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∏–µ–Ω—Ç–∞:",
                options=filtered_df['customer_unique_id'].unique()
            )
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
            customer_data = filtered_df[filtered_df['customer_unique_id'] == selected_customer].iloc[0]
            
            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–ª–∏–µ–Ω—Ç–µ
            col1, col2 = st.columns(2)
            with col1:
                st.metric("–†–µ–≥–∏–æ–Ω", customer_data['customer_state'])
                st.metric("RFM Score", customer_data['RFM_Score'])
                st.metric("–°—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫", f"{customer_data['payment_value']:.2f}")
            
            with col2:
                st.metric("–û—Ü–µ–Ω–∫–∞ –æ—Ç–∑—ã–≤–∞", customer_data['review_score'])
                is_churned = customer_data['is_churned'] == 1
                churn_label = "–î–∞" if is_churned else "–ù–µ—Ç"
                delta_text = "–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫" if is_churned else "–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫"
                st.metric("–°—Ç–∞—Ç—É—Å –æ—Ç—Ç–æ–∫–∞", 
                         churn_label,
                         delta=delta_text,
                         delta_color="inverse" if is_churned else "normal"
                         )
    
        # –í–∫–ª–∞–¥–∫–∞ 5: –ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        with tab5:
            st.header("–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤")
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
            X = filtered_df[numerical_features + categorical_features]
            y = filtered_df['is_churned']
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.3, random_state=42, stratify=y)
            
            # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
            model_option = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
                ("Logistic Regression", "Decision Tree", "XGBoost")
            )
            
            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            if st.button("–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å"):
                with st.spinner("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."):
                    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                    if model_option == "Logistic Regression":
                        model = LogisticRegression(
                            class_weight='balanced',
                            max_iter=1000,
                            random_state=42
                        )
                    elif model_option == "Decision Tree":
                        model = DecisionTreeClassifier(
                            max_depth=5,
                            min_samples_leaf=50,
                            class_weight='balanced',
                            random_state=42
                        )
                    elif model_option == "XGBoost":
                        scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)
                        model = xgb.XGBClassifier(
                            objective='binary:logistic',
                            n_estimators=150,
                            max_depth=5,
                            learning_rate=0.1,
                            subsample=0.8,
                            colsample_bytree=0.8,
                            scale_pos_weight=scale_pos_weight,
                            random_state=42,
                            n_jobs=-1
                        )
                    
                    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
                    pipeline = train_model(model, X_train, y_train)
                    st.session_state.trained_pipeline = pipeline  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–µ—Å—Å–∏–∏
                    st.session_state.model_option = model_option # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏

                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                    y_pred, y_proba = predict_churn(pipeline, X_test)
                    
                    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
                    metrics = extended_classification_report(y_test, y_pred, y_proba, model_option)
                    
                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                    st.subheader("–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏")
                    col1, col2, col3 = st.columns(3)
                    col1.metric("ROC-AUC", f"{metrics['roc_auc']:.4f}")
                    col2.metric("F1 Score", f"{metrics['f1']:.4f}")
                    col3.metric("Accuracy", f"{metrics['accuracy']:.4f}")
                    
                    col4, col5, col6 = st.columns(3)
                    col4.metric("Sensitivity", f"{metrics['sensitivity']:.4f}")
                    col5.metric("Specificity", f"{metrics['specificity']:.4f}")
                    col6.metric("Precision", f"{metrics['precision']:.4f}")
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ROC-–∫—Ä–∏–≤–æ–π
                    st.subheader("ROC-–∫—Ä–∏–≤–∞—è")
                    fpr, tpr, _ = roc_curve(y_test, y_proba)
                    fig = px.line(x=fpr, y=tpr, 
                                 labels={'x': 'False Positive Rate', 'y': 'True Positive Rate'},
                                 title=f'ROC Curve (AUC = {metrics["roc_auc"]:.2f})')
                    fig.add_shape(type='line', line=dict(dash='dash'),
                                x0=0, x1=1, y0=0, y1=1)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫
                    st.subheader("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
                    cm = confusion_matrix(y_test, y_pred)
                    fig_cm = px.imshow(cm, 
                                       labels=dict(x="Predicted", y="Actual"),
                                       x=['Not Churned', 'Churned'],
                                       y=['Not Churned', 'Churned'],
                                       text_auto=True)
                    st.plotly_chart(fig_cm, use_container_width=True)
                    
                    st.success(f"–ú–æ–¥–µ–ª—å {model_option} —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")

            # –ë–ª–æ–∫ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            if 'trained_pipeline' in st.session_state:
                st.subheader("–°–∫–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
                pipeline_to_save = st.session_state.trained_pipeline
                model_option_to_save = st.session_state.model_option
                model_filename = f'{model_option_to_save.replace(" ", "_").lower()}_model.pkl'
                
                # –°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –≤ –±–∞–π—Ç—ã
                from io import BytesIO
                model_bytes = BytesIO()
                joblib.dump(pipeline_to_save, model_bytes)
                model_bytes.seek(0)

                # –ö–Ω–æ–ø–∫–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                st.download_button(
                    label=f"–°–∫–∞—á–∞—Ç—å {model_option_to_save.replace(" ", "_")}.pkl",
                    data=model_bytes,
                    file_name=model_filename,
                    mime="application/octet-stream"
                )

            # –ë–ª–æ–∫ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç—Ç–æ–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞
            st.subheader("–ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞")
            customer_id_predict = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞:",
                options=filtered_df['customer_unique_id'].unique(),
                key="customer_predict_select"
            )
            
            if st.button("–°–¥–µ–ª–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑"):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
                if 'trained_pipeline' not in st.session_state:
                    st.error("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–æ–π –≤–∫–ª–∞–¥–∫–µ.")
                else:
                    try:
                        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Å–µ—Å—Å–∏–∏
                        pipeline = st.session_state.trained_pipeline
                        current_model_option = st.session_state.model_option
                        
                        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–∞
                        customer_data = filtered_df[filtered_df['customer_unique_id'] == customer_id_predict].iloc[0:1]
                        X_customer = customer_data[numerical_features + categorical_features]
                        
                        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                        pred, proba = predict_churn(pipeline, X_customer)
                        
                        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞")
                        col1, col2 = st.columns(2)
                        col1.metric("–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç—Ç–æ–∫–∞", f"{proba[0]:.2%}")

                        is_high_risk = pred[0] == 1
                        prognosis_label = "–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞" if is_high_risk else "–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫ –æ—Ç—Ç–æ–∫–∞"
                        delta_display_text = "‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ" if is_high_risk else "‚úÖ –ù–æ—Ä–º–∞"

                        col2.metric("–ü—Ä–æ–≥–Ω–æ–∑",
                                   prognosis_label,
                                   delta=delta_display_text,
                                   delta_color="inverse" if is_high_risk else "normal"
                                   )

                        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–¥–ª—è tree-based –º–æ–¥–µ–ª–µ–π)
                        if current_model_option in ["Decision Tree", "XGBoost"]:
                            st.subheader("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
                            try:
                                # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞
                                classifier = pipeline.named_steps['classifier']
                                
                                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
                                preprocessor_step = pipeline.named_steps['preprocessor']
                                
                                # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ OneHotEncoding
                                ohe_feature_names = preprocessor_step.transformers_[1][1]\
                                    .get_feature_names_out(categorical_features)
                                
                                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–º–µ–Ω –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                                feature_names = numerical_features + list(ohe_feature_names)
                                
                                # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                                importances = classifier.feature_importances_
                                
                                # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                                importance_df = pd.DataFrame({
                                    'feature': feature_names,
                                    'importance': importances
                                }).sort_values(by='importance', ascending=False)
                                
                                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø-15 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                                top_n = 15
                                fig_importance = px.bar(
                                    importance_df.head(top_n),
                                    x='importance',
                                    y='feature',
                                    orientation='h',
                                    title=f'–¢–æ–ø-{top_n} –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {current_model_option}'
                                )
                                st.plotly_chart(fig_importance, use_container_width=True)
                                
                            except AttributeError:
                                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {current_model_option}.")
                            except Exception as e:
                                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {str(e)}")

                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏: {str(e)}")

            # –í–∫–ª–∞–¥–∫–∞ 6: –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            with tab6:
                st.subheader("–î–æ—Ö–æ–¥ –æ—Ç –∑–∞–∫–∞–∑–æ–≤ (—Ç—ã—Å. —Ä–µ–∞–ª–æ–≤)")

                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HoloViews
                hv.extension('bokeh', logo=False)

                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                overlay_opts = dict(width=800, height=600, toolbar='above', xaxis=None, yaxis=None)
                quad_opts = dict(tools=['hover'], colorbar=True, alpha=0, hover_alpha=0.2)
                gv.opts.defaults(
                    gv.opts.Overlay(**overlay_opts),
                    gv.opts.QuadMesh(**quad_opts)
                )

                # –ó–∞–≥—Ä—É–∑–∫–∞ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                brazil = load_and_prepare_geo()

                # –ò–º—è —Å—Ç–æ–ª–±—Ü–∞ –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
                agg_name = 'geolocation_zip_code_prefix'

                T  = 0.05 # dynspread threshold
                PX = 1 # dynspread px growth

                # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–∞—Ä—Ç—ã –ø–æ—á—Ç–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
                def plotted_zipcodes(df, agg_name=agg_name, cmap=rainbow):
                    # –§–æ–Ω–æ–≤—ã–µ —Ç–∞–π–ª—ã (—Ç–µ–º–Ω–æ-—Å–µ—Ä–∞—è –∫–∞—Ä—Ç–∞)
                    url = (
                        "https://server.arcgisonline.com/ArcGIS/rest/services/"
                        "Canvas/World_Dark_Gray_Base/MapServer/tile/{Z}/{Y}/{X}.png"
                    )
                    geomap  = gv.WMTS(WMTSTileSource(url=url))

                    # –¢–æ—á–∫–∏ –≤ –ø—Ä–æ–µ–∫—Ü–∏–æ–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö
                    points  = hv.Points(gv.Dataset(df, kdims=['x', 'y'], vdims=[agg_name]))

                    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é datashader
                    agg = datashade(points,
                                    element_type=gv.Image,
                                    aggregator=ds.min(agg_name),
                                    cmap=cmap)

                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–¥–∏–º–æ—Å—Ç–∏
                    zipcodes = dynspread(agg, threshold=T, max_px=PX)

                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–ª–æ–π –¥–ª—è hover
                    hover = hv.util.Dynamic(
                        rasterize(points,
                                aggregator=ds.min(agg_name),
                                width=50,
                                height=25,
                                streams=[RangeXY]
                        ),
                        operation=hv.QuadMesh
                    ).opts(cmap=cmap)

                    return geomap * zipcodes * hover

                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –∑–∞–≥—Ä—É–∑–∫–∏
                with st.spinner('–ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Ä—Ç—É...'):
                    bokeh_fig = hv.render(plotted_zipcodes(brazil), backend='bokeh')

                # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–≥–∏–æ–Ω—É/–≥–æ—Ä–æ–¥—É
                def filter_data(df, level, name):
                    df = df[df[level] == name]
                    df = df[(df.x <= df.x.quantile(0.999)) & (df.x >= df.x.quantile(0.001))]
                    df = df[(df.y <= df.y.quantile(0.999)) & (df.y >= df.y.quantile(0.001))]
                    return df
                
                # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è DataFrame —Å –¥–æ—Ö–æ–¥–∞–º–∏
                def build_revenue_df(geo, orders):
                    geo3 = geo.set_index('geolocation_zip_code_prefix_3_digits').copy()

                    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ—á—Ç–æ–≤–æ–º—É –∏–Ω–¥–µ–∫—Å—É –∏ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–Ω
                    gp = orders.groupby('customer_zip_code_prefix_3_digits')['price'].sum()

                    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –≥–µ–æ–¥–∞–Ω–Ω—ã–º–∏
                    revenue = geo3.join(gp)
                    revenue['revenue'] = revenue['price'].fillna(0) / 1_000  # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç—ã—Å—è—á–∏

                    return revenue
                
                # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–∞—Ä—Ç—ã –¥–æ—Ö–æ–¥–æ–≤
                def map_plot(df, agg_name='revenue', cmap=fire):
                    T, PX = 0.05, 1
                    url = (
                        "https://server.arcgisonline.com/ArcGIS/rest/services/"
                        "Canvas/World_Dark_Gray_Base/MapServer/tile/{Z}/{Y}/{X}.png"
                    )
                    geomap = gv.WMTS(WMTSTileSource(url=url))

                    points = hv.Points(gv.Dataset(df, kdims=['x', 'y'], vdims=[agg_name]))

                    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                    agg = datashade(points,
                                    element_type=gv.Image,
                                    aggregator=ds.mean(agg_name),
                                    cmap=cmap)
                    img = dynspread(agg, threshold=T, max_px=PX)

                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–ª–æ–π
                    hover = hv.util.Dynamic(
                        rasterize(points,
                                aggregator=ds.mean(agg_name),
                                width=50, height=25,
                                streams=[RangeXY]),
                        operation=hv.QuadMesh
                    ).opts(cmap=cmap)

                    return geomap * img * hover

                # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
                geo = load_and_prepare_geo()
                orders = load_orders()

                # –§–∏–ª—å—Ç—Ä—ã –Ω–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
                st.sidebar.header("üìç –ö–∞—Ä—Ç–∞")
                states = ['<all brazil>'] + sorted(geo['geolocation_state'].unique())
                state_choice = st.sidebar.selectbox("state (uf)", states, index=0)

                # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≥–æ—Ä–æ–¥–æ–≤ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —à—Ç–∞—Ç–∞
                cities_in_state = sorted(geo.query("geolocation_state == @state_choice")['geolocation_city'].str.lower().unique())

                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤
                sub_geo = filter_data(geo, 'geolocation_state', state_choice)

                if state_choice == '<all brazil>':
                    sub_geo    = geo.copy()                     
                    sub_orders = orders.copy()                  
                else:
                    sub_geo    = filter_data(geo, 'geolocation_state', state_choice)
                    sub_orders = orders[orders['customer_state'] == state_choice]

                    cities_in_state = sorted(
                        sub_geo['geolocation_city'].str.lower().unique()
                    )
                    city_choice = st.sidebar.selectbox(
                        "city (optional)", ['<all>'] + cities_in_state
                    )

                    if city_choice != '<all>':
                        sub_geo = filter_data(sub_geo, 'geolocation_city', city_choice)
                        sub_orders = sub_orders[
                            sub_orders['customer_city'].str.lower() == city_choice
                        ]

                # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ DataFrame —Å –¥–æ—Ö–æ–¥–∞–º–∏
                revenue_df = build_revenue_df(sub_geo, sub_orders)

                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
                st.sidebar.markdown("### –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
                st.sidebar.metric("n¬∫ zip prefixes", len(revenue_df))
                st.sidebar.metric("total revenue (k R$)", int(revenue_df['revenue'].sum()))

                if st.sidebar.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å `describe()`", value=False):
                    st.sidebar.dataframe(revenue_df['revenue'].describe().to_frame(), use_container_width=True)

                # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –∑–∞–≥—Ä—É–∑–∫–∏
                with st.spinner("–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ datashader tiles"):
                    fig = hv.render(map_plot(revenue_df), backend='bokeh')
                streamlit_bokeh(fig, use_container_width=True)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        st.stop()

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    main()